<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SAP-C02 Day 14 Study</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', sans-serif; background: linear-gradient(135deg, #6f42c1, #5a32a3); min-height: 100vh; padding: 20px; }
        .container { max-width: 1000px; margin: 0 auto; background: white; border-radius: 15px; box-shadow: 0 20px 40px rgba(0,0,0,0.1); overflow: hidden; }
        .header { background: linear-gradient(135deg, #6f42c1, #5a32a3); color: white; padding: 30px; text-align: center; }
        .header h1 { font-size: 2.5em; margin-bottom: 10px; }
        .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(120px, 1fr)); gap: 15px; padding: 20px 30px; background: #f8f9fa; }
        .stat-card { background: white; padding: 15px; border-radius: 10px; text-align: center; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
        .stat-number { font-size: 1.8em; font-weight: bold; color: #6f42c1; }
        .question-container { padding: 30px; }
        .question-header { background: #6f42c1; color: white; padding: 15px 20px; border-radius: 10px 10px 0 0; font-weight: bold; }
        .question-content { background: #f8f9fa; padding: 25px; border: 1px solid #dee2e6; border-top: none; border-radius: 0 0 10px 10px; }
        .question-text { font-size: 1.1em; line-height: 1.6; margin-bottom: 25px; }
        .multi-choice-note { background: #fff3cd; padding: 10px; border-radius: 5px; margin-bottom: 15px; font-size: 0.9em; color: #856404; }
        .choices { list-style: none; }
        .choice { background: white; margin: 10px 0; padding: 15px; border: 2px solid #dee2e6; border-radius: 8px; cursor: pointer; transition: all 0.3s ease; }
        .choice:hover { border-color: #6f42c1; transform: translateY(-2px); }
        .choice.selected { border-color: #6f42c1; background: #f3e5f5; }
        .choice.correct { border-color: #28a745; background: #d4edda; }
        .choice.incorrect { border-color: #dc3545; background: #f8d7da; }
        .choice-letter { display: inline-block; width: 30px; height: 30px; background: #6f42c1; color: white; border-radius: 50%; text-align: center; line-height: 30px; margin-right: 15px; font-weight: bold; }
        .choice.correct .choice-letter { background: #28a745; }
        .choice.incorrect .choice-letter { background: #dc3545; }
        .explanation { background: #fff3cd; border: 1px solid #ffeaa7; border-radius: 8px; padding: 20px; margin-top: 20px; display: none; }
        .explanation.show { display: block; }
        .navigation { padding: 30px; background: #f8f9fa; display: flex; justify-content: space-between; align-items: center; }
        .btn { padding: 12px 24px; border: none; border-radius: 25px; cursor: pointer; font-weight: bold; transition: all 0.3s ease; text-decoration: none; display: inline-block; }
        .btn-primary { background: linear-gradient(135deg, #6f42c1, #5a32a3); color: white; }
        .btn-success { background: linear-gradient(135deg, #28a745, #1e7e34); color: white; }
        .btn:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0,0,0,0.15); }
        .back-buttons { text-align: center; padding: 20px; background: #f8f9fa; }
        @media (max-width: 768px) { .navigation { flex-direction: column; gap: 15px; } }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üìö SAP-C02 Day 14</h1>
            <p>18 questions for today's study session</p>
        </div>
        
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number" id="currentQ">1</div>
                <div>Current</div>
            </div>
            <div class="stat-card">
                <div class="stat-number" id="totalQ">18</div>
                <div>Total</div>
            </div>
            <div class="stat-card">
                <div class="stat-number" id="correctCount">0</div>
                <div>Correct</div>
            </div>
            <div class="stat-card">
                <div class="stat-number" id="accuracy">0%</div>
                <div>Accuracy</div>
            </div>
        </div>
        
        <div class="question-container" id="questionContainer"></div>
        
        <div class="navigation">
            <button class="btn btn-primary" onclick="previousQuestion()" id="prevBtn">Previous</button>
            <div>
                <button class="btn btn-primary" onclick="showAnswer()" id="showAnswerBtn">Show Answer</button>
                <button class="btn btn-success" onclick="nextQuestion()" id="nextBtn" style="display:none;">Next Question</button>
            </div>
            <button class="btn btn-primary" onclick="nextQuestion()" id="skipBtn">Skip</button>
        </div>
        
        <div class="back-buttons">
            <a href="../daily_study_launcher.html" class="btn btn-primary">üìö Back to Study Plan</a>
            <a href="../../../master_study_launcher.html" class="btn btn-success">üè† Master Hub</a>
            <a href="../../../index.html" class="btn" style="background: #17a2b8; color: white;">üè° Home Page</a>
        </div>
    </div>

    <script>
        const questions = [
        {
                "question_number": 235,
                "question_text": "A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared files stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the cluster size to 1.000 EC2 instances, overall performance was well below expectations.Which collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose three.)",
                "choices": [
                        "Ensure the HPC cluster is launched within a single Availability Zone.",
                        "Launch the EC2 instances and attach elastic network interfaces in multiples of four.",
                        "Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled.",
                        "Ensure the cluster is launched across multiple Availability Zones.",
                        "Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array.",
                        "Replace Amazon EFS with Amazon FSx for Lustre."
                ],
                "correct_answer": "ACF",
                "is_multiple_choice": true,
                "voting_data": {
                        "ACF": 24,
                        "CDF": 2
                }
        },
        {
                "question_number": 236,
                "question_text": "A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire organization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique tag values.Which solution will meet these requirements?",
                "choices": [
                        "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.",
                        "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the organization's management account.",
                        "Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.",
                        "Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs."
                ],
                "correct_answer": "A",
                "is_multiple_choice": false,
                "voting_data": {
                        "A": 16,
                        "B": 3
                }
        },
        {
                "question_number": 237,
                "question_text": "A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry Transport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket.Recently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new design on AWS that is highly available and scalable to prevent a similar occurrence.Which solution will meet these requirements?",
                "choices": [
                        "Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a domain name in Amazon Route 53. Create a Route 53 failover policy. Route the sensors to send the data to the domain name.",
                        "Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer (NLB) that points to the Amazon MSK broker. Enable NLB health checks. Route the sensors to send the data to the NLB.",
                        "Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core.",
                        "Deploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2 instance. Route the sensors to send the data to AWS IoT Core."
                ],
                "correct_answer": "C",
                "is_multiple_choice": false,
                "voting_data": {
                        "C": 33,
                        "B": 5
                }
        },
        {
                "question_number": 238,
                "question_text": "A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances. Amazon Elastic File System (Amazon EFS) file systems, and Amazon RDS DB instances.To meet regulatory and business requirements, the company must make the following changes for data backups:‚Ä¢\tBackups must be retained based on custom daily, weekly, and monthly requirements.‚Ä¢\tBackups must be replicated to at least one other AWS Region immediately after capture.‚Ä¢\tThe backup solution must provide a single source of backup status across the AWS environment.‚Ä¢\tThe backup solution must send immediate notifications upon failure of any resource backup.Which combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)",
                "choices": [
                        "Create an AWS Backup plan with a backup rule for each of the retention requirements.",
                        "Configure an AWS Backup plan to copy backups to another Region.",
                        "Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs.",
                        "Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED.",
                        "Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements.",
                        "Set up RDS snapshots on each database."
                ],
                "correct_answer": "ABD",
                "is_multiple_choice": true,
                "voting_data": {
                        "ABD": 18
                }
        },
        {
                "question_number": 239,
                "question_text": "A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data from a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the data and provide information back to researchers. The data platform must meet the following requirements:‚Ä¢\tProvide near-real-time analytics of the inbound genomic data‚Ä¢\tEnsure the data is flexible, parallel, and durable‚Ä¢\tDeliver results of processing to a data warehouseWhich strategy should a solutions architect use to meet these requirements?",
                "choices": [
                        "Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon RDS instance.",
                        "Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon Redshift cluster using Amazon EMR.",
                        "Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon Redshift cluster.",
                        "Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR."
                ],
                "correct_answer": "B",
                "is_multiple_choice": false,
                "voting_data": {
                        "B": 21,
                        "D": 1
                }
        },
        {
                "question_number": 240,
                "question_text": "A solutions architect needs to define a reference architecture for a solution for three-tier applications with web. application, and NoSQL data layers. The reference architecture must meet the following requirements:‚Ä¢\tHigh availability within an AWS Region‚Ä¢\tAble to fail over in 1 minute to another AWS Region for disaster recovery‚Ä¢\tProvide the most efficient solution while minimizing the impact on the user experienceWhich combination of steps will meet these requirements? (Choose three.)",
                "choices": [
                        "Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour.",
                        "Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds.",
                        "Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions.",
                        "Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario.",
                        "Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources.",
                        "Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources."
                ],
                "correct_answer": "BCE",
                "is_multiple_choice": true,
                "voting_data": {
                        "BCE": 15,
                        "ACE": 1
                }
        },
        {
                "question_number": 241,
                "question_text": "A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to connect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-premises storage. Custom applications analyze this data to detect anomalies.The number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not able to scale for peak traffic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the scaling challenges.Which solution will meet these requirements with the LEAST operational overhead?",
                "choices": [
                        "Use AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache Kafka application to store the data in Amazon S3. Use a pretrained model in Amazon SageMaker to detect anomalies.",
                        "Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies.",
                        "Use AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use the built-in machine learning transforms in AWS Glue to detect anomalies.",
                        "Use Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use Amazon Lookout for Metrics to detect anomalies."
                ],
                "correct_answer": "B",
                "is_multiple_choice": false,
                "voting_data": {
                        "B": 35,
                        "C": 5,
                        "D": 1
                }
        },
        {
                "question_number": 242,
                "question_text": "During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.Which solution will ensure that the credentials are appropriately secured automatically?",
                "choices": [
                        "Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials",
                        "Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS.",
                        "Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user.",
                        "Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user."
                ],
                "correct_answer": "D",
                "is_multiple_choice": false,
                "voting_data": {
                        "D": 28,
                        "C": 2
                }
        },
        {
                "question_number": 243,
                "question_text": "A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's information security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the minimum permissions necessary to function.To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.Which combination of steps should the solutions architect take to implement this solution? (Choose two.)",
                "choices": [
                        "Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application‚Äôs VPC. Update the bucket policy to require access from an access point.",
                        "Create an interface endpoint for Amazon S3 in each application's VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint.",
                        "Create a gateway endpoint for Amazon S3 in each application's VPConfigure the endpoint policy to allow access to an S3 access point. Specify the route table that is used to access the access point.",
                        "Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application's VPC. Update the bucket policy to require access from an access point.",
                        "Create a gateway endpoint for Amazon S3 in the data lake's VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket."
                ],
                "correct_answer": "AC",
                "is_multiple_choice": true,
                "voting_data": {
                        "AC": 46,
                        "AB": 12,
                        "AD": 4,
                        "BD": 1,
                        "AE": 1,
                        "DE": 1
                }
        },
        {
                "question_number": 244,
                "question_text": "A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that send application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.The company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring solution that uses Splunk on premises. A solutions architect needs to determine how to send networking traffic to Splunk.How should the solutions architect meet these requirements?",
                "choices": [
                        "Enable VPC flows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an Amazon S3 bucket by using the pre-defined export function. Generate ACCESS_KEY and SECRET_KEY AWS credentials. Configure Splunk to pull the logs from the S3 bucket by using those credentials.",
                        "Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function with a Kinesis Data Firehose stream processor that extracts individual log events from records sent by CloudWatch Logs subscription filters. Enable VPC flows logs, and send them to CloudWatch. Create a CloudWatch Logs subscription that sends log events to the Kinesis Data Firehose delivery stream.",
                        "Ask the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs grouped by database name. Export Athena results to another S3 bucket. Invoke an AWS Lambda function to automatically send any new file that is put in the S3 bucket to Splunk.",
                        "Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1-minute sliding window to collect the events. Create a SQL query that uses the anomaly detection template to monitor any networking traffic anomalies in near-real time. Send the result to an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination."
                ],
                "correct_answer": "B",
                "is_multiple_choice": false,
                "voting_data": {
                        "B": 34
                }
        },
        {
                "question_number": 245,
                "question_text": "A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the development teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide the information to the company's finance team.The company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States. However, some resources have been created in other Regions.A solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the accounts. The solution also must ensure that the company can create resources only in Regions in the United States.Which combination of steps will meet these requirements in the MOST operationally efficient way? (Choose three.)",
                "choices": [
                        "Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team's S3 bucket.",
                        "Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation.",
                        "Create an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the United States. Apply the SCP to the OU.",
                        "Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU.",
                        "Create an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role. Use AWS Cost Explorer and the Billing and Cost Management console to analyze cost.",
                        "Create an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role."
                ],
                "correct_answer": "BDE",
                "is_multiple_choice": true,
                "voting_data": {
                        "BDE": 34,
                        "ABD": 4,
                        "BDF": 2
                }
        },
        {
                "question_number": 246,
                "question_text": "A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires read-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security team.How should a solutions architect meet these requirements?",
                "choices": [
                        "Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a trust relationship between the IAM policy in each member account and the security account. Ask the security team to use the IAM policy to gain access.",
                        "Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.",
                        "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the management account from the security account. Use the generated temporary credentials to gain access.",
                        "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the member account from the security account. Use the generated temporary credentials to gain access."
                ],
                "correct_answer": "B",
                "is_multiple_choice": false,
                "voting_data": {
                        "B": 22
                }
        },
        {
                "question_number": 247,
                "question_text": "A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private subnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.A solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.Which set of additional steps should the solutions architect take to meet these requirements?",
                "choices": [
                        "Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet.",
                        "Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet.",
                        "Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet.",
                        "Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet."
                ],
                "correct_answer": "B",
                "is_multiple_choice": false,
                "voting_data": {
                        "B": 17,
                        "A": 1
                }
        },
        {
                "question_number": 248,
                "question_text": "An education company is running a web application used by college students around the world. The application runs in an Amazon Elastic Container Service (Amazon ECS) cluster in an Auto Scaling group behind an Application Load Balancer (ALB). A system administrator detects a weekly spike in the number of failed login attempts, which overwhelm the application's authentication service. All the failed login attempts originate from about 500 different IP addresses that change each week. A solutions architect must prevent the failed login attempts from overwhelming the authentication service.Which solution meets these requirements with the MOST operational efficiency?",
                "choices": [
                        "Use AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses.",
                        "Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB.",
                        "Use AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges.",
                        "Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB."
                ],
                "correct_answer": "B",
                "is_multiple_choice": false,
                "voting_data": {
                        "B": 24
                }
        },
        {
                "question_number": 249,
                "question_text": "A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public SFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for outbound traffic. Changes to the SFTP endpoint IP addresses are not permitted.The company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.Which solution meets these requirements?",
                "choices": [
                        "Register the customer-owned block of IP addresses in the company's AWS account. Create Elastic IP addresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWS Transfer to store the files in Amazon S3.",
                        "Add a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign them to an Application Load Balancer (ALB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the ALStore the files in attached Amazon Elastic Block Store (Amazon EBS) volumes.",
                        "Register the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load Balancer (NLB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the NLB. Store the files in Amazon S3.",
                        "Register the customer-owned block of IP addresses in the company‚Äôs AWS account. Create Elastic IP addresses from the address pool and assign them to an Amazon S3 VPC endpoint. Enable SFTP support on the S3 bucket."
                ],
                "correct_answer": "A",
                "is_multiple_choice": false,
                "voting_data": {
                        "A": 23
                }
        },
        {
                "question_number": 250,
                "question_text": "A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high-throughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the application to be fault tolerant.Which solution will meet these requirements?",
                "choices": [
                        "Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.",
                        "Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each EC2 instance.",
                        "Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking.",
                        "Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance."
                ],
                "correct_answer": "A",
                "is_multiple_choice": false,
                "voting_data": {
                        "A": 18
                }
        },
        {
                "question_number": 251,
                "question_text": "A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon API Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.After initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The company believes this traffic is originating from a botnet and wants to secure its API while minimizing cost.Which approach should the company take to secure its API?",
                "choices": [
                        "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Configure CloudFront with an origin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only the OAI can run the POST method.",
                        "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Add a custom header to the CloudFront distribution populated with an API key. Configure the API to require an API key on the POST method.",
                        "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a resource policy with a request limit and associate it with the API. Configure the API to require an API key on the POST method.",
                        "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan."
                ],
                "correct_answer": "D",
                "is_multiple_choice": false,
                "voting_data": {
                        "D": 29,
                        "C": 1
                }
        },
        {
                "question_number": 252,
                "question_text": "A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor all data activity on all the databases.Which solution will achieve this goal?",
                "choices": [
                        "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch Service cluster for further analysis.",
                        "Start a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program the Lambda function to decrypt the messages from EventBridge and to publish all database activity to Amazon S3 for further analysis.",
                        "Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis.",
                        "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. Run queries on the Amazon Redshift data to determine database activities on the Aurora database."
                ],
                "correct_answer": "C",
                "is_multiple_choice": false,
                "voting_data": {
                        "C": 16
                }
        }
];
        
        let currentQuestionIndex = 0;
        let correctAnswers = 0;
        let answeredQuestions = new Set();
        let selectedAnswers = {};
        
        function loadQuestion() {
            const question = questions[currentQuestionIndex];
            if (!question) return;
            
            const container = document.getElementById('questionContainer');
            const isMultipleChoice = question.is_multiple_choice;
            
            let choicesHtml = '';
            question.choices.forEach((choice, index) => {
                const letter = String.fromCharCode(65 + index);
                choicesHtml += `
                    <li class="choice" onclick="selectChoice('${letter}', this)" data-choice="${letter}">
                        <span class="choice-letter">${letter}</span>
                        ${choice}
                    </li>
                `;
            });
            
            const multiChoiceNote = isMultipleChoice ? 
                '<div class="multi-choice-note">üìù Multiple Choice: You can select multiple answers</div>' : '';
            
            container.innerHTML = `
                <div class="question-header">
                    Question ${question.question_number} (Day 14 - ${currentQuestionIndex + 1} of ${questions.length})
                </div>
                <div class="question-content">
                    <div class="question-text">${question.question_text}</div>
                    ${multiChoiceNote}
                    <ul class="choices">${choicesHtml}</ul>
                    <div class="explanation" id="explanation"></div>
                </div>
            `;
            
            updateStats();
        }
        
        function selectChoice(letter, element) {
            const question = questions[currentQuestionIndex];
            const isMultipleChoice = question.is_multiple_choice;
            
            if (!selectedAnswers[currentQuestionIndex]) {
                selectedAnswers[currentQuestionIndex] = [];
            }
            
            if (isMultipleChoice) {
                // Multiple choice - toggle selection
                if (element.classList.contains('selected')) {
                    element.classList.remove('selected');
                    selectedAnswers[currentQuestionIndex] = selectedAnswers[currentQuestionIndex].filter(ans => ans !== letter);
                } else {
                    element.classList.add('selected');
                    selectedAnswers[currentQuestionIndex].push(letter);
                }
            } else {
                // Single choice - clear others first
                document.querySelectorAll('.choice').forEach(choice => {
                    choice.classList.remove('selected');
                });
                element.classList.add('selected');
                selectedAnswers[currentQuestionIndex] = [letter];
            }
        }
        
        function showAnswer() {
            const question = questions[currentQuestionIndex];
            const selectedAnswer = selectedAnswers[currentQuestionIndex];
            
            if (!selectedAnswer || selectedAnswer.length === 0) {
                alert('Please select an answer first!');
                return;
            }
            
            answeredQuestions.add(currentQuestionIndex);
            
            document.querySelectorAll('.choice').forEach(choice => {
                const choiceLetter = choice.getAttribute('data-choice');
                if (question.correct_answer.includes(choiceLetter)) {
                    choice.classList.add('correct');
                } else if (selectedAnswer.includes(choiceLetter)) {
                    choice.classList.add('incorrect');
                }
            });
            
            let isCorrect = false;
            if (question.is_multiple_choice) {
                const correctAnswers = question.correct_answer.split('').sort().join('');
                const userAnswers = selectedAnswer.sort().join('');
                isCorrect = correctAnswers === userAnswers;
            } else {
                isCorrect = selectedAnswer[0] === question.correct_answer;
            }
            
            if (isCorrect) {
                correctAnswers++;
            }
            
            const explanationDiv = document.getElementById('explanation');
            const yourAnswer = selectedAnswer.join(', ');
            explanationDiv.innerHTML = `<strong>Correct Answer:</strong> ${question.correct_answer}<br><strong>Your Answer:</strong> ${yourAnswer}<br><strong>Result:</strong> ${isCorrect ? '‚úÖ Correct' : '‚ùå Incorrect'}`;
            explanationDiv.classList.add('show');
            
            document.getElementById('showAnswerBtn').style.display = 'none';
            document.getElementById('nextBtn').style.display = 'inline-block';
            
            updateStats();
        }
        
        function nextQuestion() {
            if (currentQuestionIndex < questions.length - 1) {
                currentQuestionIndex++;
                loadQuestion();
                document.getElementById('showAnswerBtn').style.display = 'inline-block';
                document.getElementById('nextBtn').style.display = 'none';
            } else {
                const accuracy = Math.round((correctAnswers / answeredQuestions.size) * 100);
                alert(`Day 14 Complete!\n\nCorrect: ${correctAnswers}/${answeredQuestions.size}\nAccuracy: ${accuracy}%\n\nGreat job!`);
            }
        }
        
        function previousQuestion() {
            if (currentQuestionIndex > 0) {
                currentQuestionIndex--;
                loadQuestion();
                document.getElementById('showAnswerBtn').style.display = 'inline-block';
                document.getElementById('nextBtn').style.display = 'none';
            }
        }
        
        function updateStats() {
            document.getElementById('currentQ').textContent = currentQuestionIndex + 1;
            document.getElementById('totalQ').textContent = questions.length;
            document.getElementById('correctCount').textContent = correctAnswers;
            const accuracy = answeredQuestions.size > 0 ? Math.round((correctAnswers / answeredQuestions.size) * 100) : 0;
            document.getElementById('accuracy').textContent = accuracy + '%';
        }
        
        // Initialize
        loadQuestion();
    </script>
</body>
</html>