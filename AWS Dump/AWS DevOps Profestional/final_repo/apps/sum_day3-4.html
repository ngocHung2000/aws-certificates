<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AWS DevOps Study App</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: #f5f5f5; }
        .container { max-width: 900px; margin: 0 auto; padding: 20px; }
        .header { background: #232f3e; color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px; }
        .progress-bar { background: #ddd; height: 10px; border-radius: 5px; margin: 10px 0; }
        .progress { background: #ff9900; height: 100%; border-radius: 5px; transition: width 0.3s; }
        .question-card { background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); margin-bottom: 20px; }
        .question-text { font-size: 18px; line-height: 1.6; margin-bottom: 20px; }
        .question-type { background: #e3f2fd; color: #1976d2; padding: 8px 16px; border-radius: 20px; font-size: 14px; font-weight: bold; margin-bottom: 15px; display: inline-block; }
        .options { margin: 20px 0; }
        .option { background: #f8f9fa; border: 2px solid #e9ecef; padding: 15px; margin: 10px 0; border-radius: 8px; cursor: pointer; transition: all 0.3s; position: relative; }
        .option:hover { border-color: #ff9900; }
        .option.selected { border-color: #ff9900; background: #fff3cd; }
        .option.correct { border-color: #28a745; background: #d4edda; }
        .option.incorrect { border-color: #dc3545; background: #f8d7da; }
        .option.partial { border-color: #ffc107; background: #fff3cd; }
        .option-checkbox { position: absolute; right: 15px; top: 50%; transform: translateY(-50%); width: 20px; height: 20px; }
        .controls { display: flex; gap: 15px; justify-content: center; margin-top: 20px; }
        .btn { padding: 12px 24px; border: none; border-radius: 6px; cursor: pointer; font-size: 16px; transition: all 0.3s; }
        .btn-primary { background: #ff9900; color: white; }
        .btn-primary:hover { background: #e68900; }
        .btn-secondary { background: #6c757d; color: white; }
        .btn-secondary:hover { background: #545b62; }
        .btn:disabled { background: #ccc; cursor: not-allowed; }
        .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin-bottom: 20px; }
        .stat-card { background: white; padding: 20px; border-radius: 8px; text-align: center; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }
        .stat-number { font-size: 24px; font-weight: bold; color: #ff9900; }
        .review-section { background: white; padding: 20px; border-radius: 10px; margin-top: 20px; }
        .wrong-answer { background: #fff3cd; padding: 15px; margin: 10px 0; border-radius: 8px; border-left: 4px solid #ffc107; }
        .hidden { display: none; }
        .explanation { margin-top: 20px; padding: 15px; border-radius: 8px; }
        .explanation.correct { background: #d4edda; border-left: 4px solid #28a745; }
        .explanation.incorrect { background: #f8d7da; border-left: 4px solid #dc3545; }
        .explanation.partial { background: #fff3cd; border-left: 4px solid #ffc107; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üöÄ AWS DevOps Professional Study App</h1>
            <div class="progress-bar">
                <div class="progress" id="progressBar"></div>
            </div>
            <p>Ng√†y <span id="currentDay">3</span> - C√¢u <span id="currentQuestion">1</span>/<span id="totalQuestions">0</span></p>
        </div>

        <div class="stats">
            <div class="stat-card">
                <div class="stat-number" id="correctCount">0</div>
                <div>C√¢u ƒë√∫ng</div>
            </div>
            <div class="stat-card">
                <div class="stat-number" id="wrongCount">0</div>
                <div>C√¢u sai</div>
            </div>
            <div class="stat-card">
                <div class="stat-number" id="accuracy">0%</div>
                <div>ƒê·ªô ch√≠nh x√°c</div>
            </div>
            <div class="stat-card">
                <div class="stat-number" id="timeSpent">0:00</div>
                <div>Th·ªùi gian</div>
            </div>
        </div>

        <div id="studyMode" class="question-card">
            <div class="question-type" id="questionType">Single Choice</div>
            <div class="question-text" id="questionText">
                ƒêang t·∫£i c√¢u h·ªèi...
            </div>
            <div class="options" id="optionsContainer">
                <!-- Options will be loaded here -->
            </div>
            <div class="controls">
                <button class="btn btn-secondary" onclick="previousQuestion()">‚¨ÖÔ∏è C√¢u tr∆∞·ªõc</button>
                <button class="btn btn-primary" onclick="submitAnswer()" id="submitBtn" disabled>X√°c nh·∫≠n</button>
                <button class="btn btn-secondary" onclick="nextQuestion()" id="nextBtn" disabled>C√¢u ti·∫øp ‚û°Ô∏è</button>
            </div>
        </div>

        <div id="reviewMode" class="review-section hidden">
            <h2>üìä K·∫øt qu·∫£ h·ªçc t·∫≠p</h2>
            <div id="reviewStats"></div>
            <div id="wrongAnswers"></div>
            <div class="controls">
                <button class="btn btn-primary" onclick="restartStudy()">üîÑ H·ªçc l·∫°i</button>
                <button class="btn btn-secondary" onclick="exportResults()">üíæ Xu·∫•t k·∫øt qu·∫£</button>
            </div>
        </div>
    </div>

    <script>
        // Sample questions with both single and multiple choice
        let questions = [
    {
        "id": 1,
        "text": "A company is refactoring applications to use AWS. The company identifies an internal web application that needs to make Amazon S3 API calls in a specific AWS account.The company wants to use its existing identity provider (IdP) auth.company.com for authentication. The IdP supports only OpenID Connect (OIDC). A DevOps engineer needs to secure the web application's access to the AWS account.Which combination of steps will meet these requirements? (Choose three.)",
        "options": [
            "Create an IAM IdP by using the provider URL, audience, and signature from the existing IP.",
            "Configure AWS IAM Identity Center (AWS Single Sign-On). Configure an IdP. Upload the IdP metadata from the existing IdP.",
            "Configure the web application to use the AssumeRoleWithWebIdentity API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls.",
            "Create an IAM role that has a policy that allows the necessary S3 actions. Configure the role's trust policy to allow the OIDC IP to assume the role if the auth.company.com:aud context key is appid_from_idp.",
            "Create an IAM role that has a policy that allows the necessary S3 actions. Configure the role's trust policy to allow the OIDC IP to assume the role if the sts.amazon.com:aud context key is appid_from_idp.",
            "Configure the web application to use the GetFederationToken API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls."
        ],
        "correct": [
            0,
            2,
            3
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 2,
        "text": "A company's production environment uses an AWS CodeDeploy blue/green deployment to deploy an application. The deployment incudes Amazon EC2 Auto Scaling groups that launch instances that run Amazon Linux 2.A working appspec.yml file exists in the code repository and contains the following text:A DevOps engineer needs to ensure that a script downloads and installs a license file onto the instances before the replacement instances start to handle request traffic. The DevOps engineer adds a hooks section to the appspec.yml file.Which hook should the DevOps engineer use to run the script that downloads and installs the license file?",
        "options": [
            "BeforeInstall",
            "DownloadBundle",
            "BeforeBlockTraffic",
            "AfterBlockTraffic"
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 3,
        "text": "A company runs applications in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster uses an Application Load Balancer to route traffic to the applications that run in the cluster.A new application that was migrated to the EKS cluster is performing poorly. All the other applications in the EKS cluster maintain appropriate operation. The new application scales out horizontally to the preconfigured maximum number of pods immediately upon deployment, before any user traffic routes to the web application.Which solution will resolve the scaling behavior of the web application in the EKS cluster?",
        "options": [
            "Implement the Vertical Pod Autoscaler in the EKS cluster.",
            "Implement the Cluster Autoscaler.",
            "Implement the AWS Load Balancer Controller in the EKS cluster.",
            "Implement the Horizontal Pod Autoscaler in the EKS cluster."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 4,
        "text": "A company is building a new pipeline by using AWS CodePipeline and AWS CodeBuild in a build account. The pipeline consists of two stages. The first stage is a CodeBuild job to build and package an AWS Lambda function. The second stage consists of deployment actions that operate on two different AWS accounts: a development environment account and a production environment account. The deployment stages use the AWS CloudFormation action that CodePipeline invokes to deploy the infrastructure that the Lambda function requires.A DevOps engineer creates the CodePipeline pipeline and configures the pipeline to encrypt build artifacts by using the AWS Key Management Service (AWS KMS) AWS managed key for Amazon S3 (the aws/s3 key). The artifacts are stored in an S3 bucket. When the pipeline runs, the CloudFormation actions fail with an access denied error.Which combination of actions must the DevOps engineer perform to resolve this error? (Choose two.)",
        "options": [
            "Create an S3 bucket in each AWS account for the artifacts. Allow the pipeline to write to the S3 buckets. Create a CodePipeline S3 action to copy the artifacts to the S3 bucket in each AWS account. Update the CloudFormation actions to reference the artifacts S3 bucket in the production account.",
            "Create a customer managed KMS key. Configure the KMS key policy to allow the IAM roles used by the CloudFormation action to perform decrypt operations. Modify the pipeline to use the customer managed KMS key to encrypt artifacts.",
            "In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, configure the CodePipeline CloudFormation action to use the roles.",
            "In the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, modify the artifacts S3 bucket policy to allow the roles access. Configure the CodePipeline CloudFormation action to use the roles.",
            "Create an AWS managed KMS key. Configure the KMS key policy to allow the development account and the production account to perform decrypt operations. Modify the pipeline to use the KMS key to encrypt artifacts."
        ],
        "correct": [
            1,
            3
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 5,
        "text": "A company requires its internal business teams to launch resources through pre-approved AWS CloudFormation templates only. The security team requires automated monitoring when resources drift from their expected state.Which strategy should be used to meet these requirements?",
        "options": [
            "Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use AWS Config rules to detect when resources have drifted from their expected state.",
            "Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a launch constraint. Use AWS Config rules to detect when resources have drifted from their expected state.",
            "Allow users to deploy CloudFormation stacks using a CloudFormation service role only. Use CloudFormation drift detection to detect when resources have drifted from their expected state.",
            "Allow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a template constraint. Use Amazon EventBridge notifications to detect when resources have drifted from their expected state."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 6,
        "text": "A company is using AWS to run digital workloads. Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations.The company wants to enforce security standards across the entire organization. To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation. A production support team can modify resources in the production environment by using the AWS Management Console to troubleshoot and resolve application-related issues.A DevOps engineer must implement a solution to identify in near real time any AWS service misconfiguration that results in noncompliance. The solution must automatically remediate the issue within 15 minutes of identification. The solution also must track noncompliant resources and events in a centralized dashboard with accurate timestamps.Which solution will meet these requirements with the LEAST development overhead?",
        "options": [
            "Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon CloudWatch Logs to identify noncompliant resources. Use CloudWatch Logs filters for drift detection. Use Amazon EventBridge to invoke the Lambda function for remediation. Stream filtered CloudWatch logs to Amazon OpenSearch Service. Set up a dashboard on OpenSearch Service for tracking.",
            "Turn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon Athena to identify noncompliant resources. Use AWS Step Functions to track query results on Athena for drift detection and to invoke an AWS Lambda function for remediation. For tracking, set up an Amazon QuickSight dashboard that uses Athena as the data source.",
            "Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources. Enable AWS Security Hub with the --no-enable-default-standards option in all the AWS accounts. Set up AWS Config managed rules and custom rules. Set up automatic remediation by using AWS Config conformance packs. For tracking, set up a dashboard on Security Hub in a designated Security Hub administrator account.",
            "Use CloudFormation drift detection to identify noncompliant resources. Use drift detection events from CloudFormation to invoke an AWS Lambda function for remediation. Configure the Lambda function to publish logs to an Amazon CloudWatch Logs log group. Configure an Amazon CloudWatch dashboard to use the log group for tracking."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 7,
        "text": "A DevOps team is merging code revisions for an application that uses an Amazon RDS Multi-AZ DB cluster for its production database. The DevOps team uses continuous integration to periodically verify that the application works. The DevOps team needs to test the changes before the changes are deployed to the production database.Which solution will meet these requirements?",
        "options": [
            "Use a buildspec file in AWS CodeBuild to restore the DB cluster from a snapshot of the production database, run integration tests, and drop the restored database after verification.",
            "Deploy the application to production. Configure an audit log of data control language (DCL) operations to capture database activities to perform if verification fails.",
            "Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates. Fail over to the standby instance if verification fails.",
            "Create a snapshot of the DB cluster before deploying the application. Use the Update requires:Replacement property on the DB instance in AWS CloudFormation to deploy the application and apply the changes."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 8,
        "text": "A company is testing a web application that runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company uses a blue/green deployment process with immutable instances when deploying new software.During testing, users are being automatically logged out of the application at random times. Testers also report that, when a new version of the application is deployed, all users are logged out. The development team needs a solution to ensure users remain logged in across scaling events and application deployments.What is the MOST operationally efficient way to ensure users remain logged in?",
        "options": [
            "Enable session sharing on the load balancer and modify the application to read from the session store.",
            "Modify the application to store user session information in an Amazon ElastiCache cluster.",
            "Enable smart sessions on the load balancer and modify the application to check for an existing session.",
            "Store user session information in an Amazon S3 bucket and modify the application to read session information from the bucket."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 9,
        "text": "A company is examining its disaster recovery capability and wants the ability to switch over its daily operations to a secondary AWS Region. The company uses AWS CodeCommit as a source control tool in the primary Region.A DevOps engineer must provide the capability for the company to develop code in the secondary Region. If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration.Which solution will meet these requirements?",
        "options": [
            "Create an Amazon S3 bucket in the secondary Region. Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket. Create an AWS Lambda function that initiates the Fargate task. Create an Amazon EventBridge rule that reacts to merge events in the CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function.",
            "Create an AWS CodeArtifact repository in the secondary Region. Create an AWS CodePipeline pipeline that uses the primary Region‚Äôs CodeCommit repository for the source action. Create a cross-Region stage in the pipeline that packages the CodeCommit repository contents and stores the contents in the CodeArtifact repository when a pull request is merged into the CodeCommit repository.",
            "Create a CodeCommit repository in the secondary Region. Create an AWS CodeBuild project to perform a Git mirror operation of the primary Region's CodeCommit repository to the secondary Region's CodeCommit repository. Create an AWS Lambda function that invokes the CodeBuild project. Create an Amazon EventBridge rule that reacts to merge events in the primary Region's CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function.",
            "Create an AWS Cloud9 environment and a CodeCommit repository in the secondary Region. Configure the primary Region's CodeCommit repository as a remote repository in the AWS Cloud9 environment. Connect the secondary Region's CodeCommit repository to the AWS Cloud9 environment."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 10,
        "text": "A company uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications. These templates must be deployed in a specific order. The company is making more changes to the templates than previously expected and wants to deploy new templates more efficiently. Additionally, the data engineering team must be notified of all changes to the templates.What should the company do to accomplish these goals?",
        "options": [
            "Implement CloudFormation StackSets and use drift detection to trigger update alerts to the data engineering team.",
            "Create an AWS Lambda function to deploy the CloudFormation templates in the required order. Use stack policies to alert the data engineering team.",
            "Leverage CloudFormation nested stacks and stack sets for deployments. Use Amazon SNS to notify the data engineering team.",
            "Host the CloudFormation templates in Amazon S3. Use Amazon S3 events to directly trigger CloudFormation updates and Amazon SNS notifications."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 11,
        "text": "A company has multiple development teams in different business units that work in a shared single AWS account. All Amazon EC2 resources that are created in the account must include tags that specify who created the resources. The tagging must occur within the first hour of resource creation.A DevOps engineer needs to add tags to the created resources that include the user ID that created the resource and the cost center ID. The DevOps engineer configures an AWS Lambda function with the cost center mappings to tag the resources. The DevOps engineer also sets up AWS CloudTrail in the AWS account. An Amazon S3 bucket stores the CloudTrail event logs.Which solution will meet the tagging requirements?",
        "options": [
            "Create a recurring hourly Amazon EventBridge scheduled rule that invokes the Lambda function. Modify the Lambda function to read the logs from the S3 bucket.",
            "Create an S3 event notification on the S3 bucket to invoke the Lambda function for s3:ObjectTagging:Put events. Enable bucket versioning on the S3 bucket.",
            "Create an Amazon EventBridge rule that uses Amazon EC2 as the event source. Configure the rule to match events delivered by CloudTrail. Configure the rule to target the Lambda function.",
            "Enable server access logging on the S3 bucket. Create an S3 event notification on the S3 bucket for s3:ObjectTagging:* events."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 12,
        "text": "A company is building a web and mobile application that uses a serverless architecture powered by AWS Lambda and Amazon API Gateway. The company wants to fully automate the backend Lambda deployment based on code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.The deployment must have the following:‚Ä¢ Separate environment pipelines for testing and production‚Ä¢ Automatic deployment that occurs for test environments onlyWhich steps should be taken to meet these requirements?",
        "options": [
            "Configure a new AWS CodePipeline service. Create a CodeCommit repository for each environment. Set up CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.",
            "Create an AWS CodeBuild configuration for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Push the Lambda function code to an Amazon S3 bucket. Set up the deployment step to deploy the Lambda functions from the S3 bucket.",
            "Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create a CodeCommit repository for each environment. Set up each CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.",
            "Create two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Set up each CodePipeline to retrieve the source code from the appropriate branch in the repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 13,
        "text": "A company has deployed a complex container-based workload on AWS. The workload uses Amazon Managed Service for Prometheus for monitoring. The workload runs in an AmazonElastic Kubernetes Service (Amazon EKS) cluster in an AWS account.The company‚Äôs DevOps team wants to receive workload alerts by using the company‚Äôs Amazon Simple Notification Service (Amazon SNS) topic. The SNS topic is in the same AWS account as the EKS cluster.Which combination of steps will meet these requirements? (Choose three.)",
        "options": [
            "Use the Amazon Managed Service for Prometheus remote write URL to send alerts to the SNS topic",
            "Create an alert manager configuration for the SNS topic.",
            "Modify the access policy of the SNS topic. Grant the aps.amazonaws.com service principal the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.",
            "Create an alerting rule that checks the availability of each of the workload‚Äôs containers.",
            "Modify the IAM role that Amazon Managed Service for Prometheus uses. Grant the role the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.",
            "Create an OpenID Connect (OIDC) provider for the EKS cluster. Create a cluster service account. Grant the account the sns:Publish permission and the sns:GetTopicAttributes permission by using an IAM role."
        ],
        "correct": [
            1,
            2,
            3
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 14,
        "text": "An ecommerce company uses a large number of Amazon Elastic Block Store (Amazon EBS) backed Amazon EC2 instances. To decrease manual work across all the instances, a DevOps engineer is tasked with automating restart actions when EC2 instance retirement events are scheduled.How can this be accomplished?",
        "options": [
            "Reboot all EC2 instances during an approved maintenance window that is outside of standard business hours. Set up Amazon CloudWatch alarms to send a notification in case any instance is failing EC2 instance status checks.",
            "Set up an AWS Health Amazon EventBridge rule to run AWS Systems Manager Automation runbooks that stop and start the EC2 instance when a retirement scheduled event occurs.",
            "Create a scheduled Amazon EventBridge rule to run an AWS Systems Manager Automation runbook that checks if any EC2 instances are scheduled for retirement once a week. If the instance is scheduled for retirement, the runbook will hibernate the instance.",
            "Enable EC2 Auto Recovery on all of the instances. Create an AWS Config rule to limit the recovery to occur during a maintenance window only."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 15,
        "text": "A DevOps engineer has implemented a CI/CD pipeline to deploy an AWS CloudFormation template that provisions a web application. The web application consists of an Application Load Balancer (ALB), a target group, a launch template that uses an Amazon Linux 2 AMI, an Auto Scaling group of Amazon EC2 instances, a security group, and an Amazon RDS for MySQL database. The launch template includes user data that specifies a script to install and start the application.The initial deployment of the application was successful. The DevOps engineer made changes to update the version of the application with the user data. The CI/CD pipeline has deployed a new version of the template. However, the health checks on the ALB are now failing. The health checks have marked all targets as unhealthy.During investigation, the DevOps engineer notices that the CloudFormation stack has a status of UPDATE_COMPLETE. However, when the DevOps engineer connects to one of the EC2 instances and checks /var/log/messages, the DevOps engineer notices that the Apache web server failed to start successfully because of a configuration error.How can the DevOps engineer ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running?",
        "options": [
            "Create an Amazon CloudWatch alarm for the UnhealthyHostCount metric. Include an appropriate alarm threshold for the target group. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation.",
            "Create a lifecycle hook on the Auto Scaling group by using the AWS::AutoScaling::LifecycleHook resource. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Set an appropriate timeout on the lifecycle hook.",
            "Use the Amazon CloudWatch agent to stream the cloud-init logs. Create a subscription filter that includes an AWS Lambda function with an appropriate invocation timeout. Configure the Lambda function to use the SignalResource API operation to signal success or failure to CloudFormation.",
            "Use the cfn-signal helper script to signal success or failure to CloudFormation. Use the WaitOnResourceSignals update policy within the CloudFormation template. Set an appropriate timeout for the update policy."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 16,
        "text": "A DevOps engineer needs to configure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2 instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling group.The DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has created a launch template and an Auto Scaling group for the green environment. Each Auto Scaling group deploys to a matching blue or green target group. The target group also specifies which software, blue or green, gets loaded on the EC2 instances. The ALB can be configured to send traffic to the blue environment‚Äôs target group or the green environment‚Äôs target group. An Amazon Route 53 record for www.example.com points to the ALB.The deployment must move traffic all at once between the software on the blue environment‚Äôs EC2 instances to the newly deployed software on the green environment‚Äôs EC2 instances.What should the DevOps engineer do to meet these requirements?",
        "options": [
            "Update the launch template to deploy the green environment‚Äôs software on the blue environment‚Äôs EC2 instances. Keep the target groups and Auto Scaling groups unchanged in both environments. Perform a rolling restart of the blue environment‚Äôs EC2 instances.",
            "Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment‚Äôs EC2 instances. When the rolling restart is complete, update the Route 53 DNS to point to the green environment‚Äôs endpoint on the ALB.",
            "Start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment‚Äôs EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send traffic to the green environment‚Äôs target group.",
            "Use an AWS CLI command to update the ALB to send traffic to the green environment‚Äôs target group. Then start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment‚Äôs EC2 instances."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 17,
        "text": "A development team manually builds an artifact locally and then places it in an Amazon S3 bucket. The application has a local cache that must be cleared when a deployment occurs. The team runs a command to do this, downloads the artifact from Amazon S3, and unzips the artifact to complete the deployment.A DevOps team wants to migrate to a CI/CD process and build in checks to stop and roll back the deployment when a failure occurs. This requires the team to track the progression of the deployment.Which combination of actions will accomplish this? (Choose three.)",
        "options": [
            "Allow developers to check the code into a code repository. Using Amazon EventBridge, on every pull into the main branch, invoke an AWS Lambda function to build the artifact and store it in Amazon S3.",
            "Set up AWS CodePipeline to deploy the application. Allow developers to check the code into a code repository as a source for the pipeline.",
            "Create a custom script to clear the cache. Specify the script in the BeforeInstall lifecycle hook in the AppSpec file.",
            "Use AWS CodeBuild to build the artifact and place it in Amazon S3. Use AWS CodeDeploy to deploy the artifact to Amazon EC2 instances.",
            "Use AWS Systems Manager to fetch the artifact from Amazon S3 and deploy it to all the instances.",
            "Create user data for each Amazon EC2 instance that contains the clear cache script. Once deployed, test the application. If it is not successful, deploy it again."
        ],
        "correct": [
            1,
            2,
            3
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 18,
        "text": "A DevOps engineer is building a solution that uses Amazon Simple Queue Service (Amazon SQS) standard queues. The solution also includes an AWS Lambda function and an Amazon DynamoDB table. The Lambda function pulls content from an SQS queue event source and writes the content to the DynamoDB table.The solution must maximize the scalability of Lambda and must prevent successfully processed SQS messages from being processed multiple times.Which solution will meet these requirements?",
        "options": [
            "Include the ReportBatchItemFailures value in the FunctionResponseTypes list in the Lambda function's event source mapping.",
            "Set the queue visibility timeout on the Lambda function's event source mapping to account for invocation throttling of the Lambda function.",
            "Decrease the batch size to 1 when configuring the Lambda function's event source mapping.",
            "Decrease the batch window to 1 second when configuring the Lambda function's event source mapping."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 19,
        "text": "A company uses an organization in AWS Organizations to manage several AWS accounts that the company's developers use. The company requires all data to be encrypted in transit.Multiple Amazon S3 buckets that were created in developer accounts allow unencrypted connections. A DevOps engineer must enforce encryption of data in transit for all existing S3 buckets that are created in accounts in the organization.Which solution will meet these requirements?",
        "options": [
            "Use AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all outbound requests from the AWS environment through the firewall. Deploy a policy to block access to all outbound requests on port 80.",
            "Use AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all inbound requests to the AWS environment through the firewall. Deploy a policy to block access to all inbound requests on port 80.",
            "Turn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the s3:x-amz-server-side-encryption-aws-kms-key-id condition key is null.",
            "Turn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the aws:SecureTransport condition key is false."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 20,
        "text": "A company is reviewing its IAM policies. One policy written by the DevOps engineer has been flagged as too permissive. The policy is used by an AWS Lambda function that issues a stop command to Amazon EC2 instances tagged with Environment: NonProduction over the weekend. The current policy is:What changes should the engineer make to achieve a policy of least permission? (Choose three.)",
        "options": [
            "Add the following conditional expression:",
            "Add the following conditional expression:",
            "Change \"Resource\": \"*\"to \"Resource\": \"arn:aws:ec2:*:*:instance/*\"",
            "Change \"Action\": \"ec2:*\"to \"Action\": \"ec2:StopInstances\"",
            "Add the following conditional expression:",
            "Add the following conditional expression:"
        ],
        "correct": [
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 21,
        "text": "A company manually provisions IAM access for its employees. The company wants to replace the manual process with an automated process. The company has an existing Active Directory system configured with an external SAML 2.0 identity provider (IdP).The company wants employees to use their existing corporate credentials to access AWS. The groups from the existing Active Directory system must be available for permission management in AWS Identity and Access Management (IAM). A DevOps engineer has completed the initial configuration of AWS IAM Identity Center (AWS Single Sign-On) in the company‚Äôs AWS account.What should the DevOps engineer do next to meet the requirements?",
        "options": [
            "Configure an AD Connector as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.",
            "Configure an external IdP as an identity source Configure automatic provisioning of users and groups by using the SAML protocol.",
            "Configure AWS Directory Service as an identity source. Configure automatic provisioning of users and groups by using the SAML protocol.",
            "Configure an external IdP as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 22,
        "text": "A company uses an organization in AWS Organizations to manage its AWS accounts. The company's automation account contains a CI/CD pipeline that creates and configures new AWS accounts.The company has a group of internal service teams that provide services to accounts in the organization. The service teams operate out of a set of services accounts. The service teams want to receive an AWS CloudTrail event in their services accounts when the CreateAccount API call creates a new account.How should the company share this CloudTrail event with the service accounts?",
        "options": [
            "Create a custom Amazon EventBridge event bus in the automation account and the services accounts. Create an EventBridge rule and policy that connects the custom event buses that are in the automation account and the services accounts.",
            "Create a custom Amazon EventBridge event bus in the services accounts. Update the custom event bus to allow events from the automation account. Create an EventBridge rule in the services account that directly listens to CloudTrail events from the automation account.",
            "Create a custom Amazon EventBridge event bus in the automation account. Create an EventBridge rule and policy that connects the custom event bus to the default event buses in the services accounts.",
            "Create an Amazon EventBridge rule in the automation account to send account creation events to the default event bus in the services accounts. Update the default event bus in the services accounts to allow events from the automation account."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 23,
        "text": "A company is storing 100 GB of log data in .csv format in an Amazon S3 bucket. SQL developers want to query this data and generate graphs to visualize it. The SQL developers also need an efficient, automated way to store metadata from the .csv file.Which combination of steps will meet these requirements with the LEAST amount of effort? (Choose three.)",
        "options": [
            "Filter the data through AWS X-Ray to visualize the data.",
            "Use the AWS Glue Data Catalog as the persistent metadata store.",
            "Filter the data through Amazon QuickSight to visualize the data.",
            "Use Amazon DynamoDB as the persistent metadata store.",
            "Query the data with Amazon Redshift.",
            "Query the data with Amazon Athena."
        ],
        "correct": [
            1,
            2,
            5
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 24,
        "text": "An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated with AWS CodeDeploy to deploy versions of an application to multiple Amazon EC2 instances for each CodePipeline stage.During a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants to improve monitoring and notifications during deployment to decrease resolution times.What should the DevOps engineer do to create notifications when issues are discovered?",
        "options": [
            "Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an Amazon Inspector assessment target to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.",
            "Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.",
            "Implement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.",
            "Implement Amazon CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Config rule to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 25,
        "text": "A company has configured an Amazon S3 event source on an AWS Lambda function. The company needs the Lambda function to run when a new object is created or an existing object is modified in a particular S3 bucket. The Lambda function will use the S3 bucket name and the S3 object key of the incoming event to read the contents of the created or modified S3 object. The Lambda function will parse the contents and save the parsed contents to an Amazon DynamoDB table.The Lambda function's execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are modified.Which solution will resolve this problem?",
        "options": [
            "Provision space in the /tmp folder of the Lambda function to give the function the ability to process large files from the S3 bucket.",
            "Create a resource policy on the Lambda function to grant Amazon S3 the permission to invoke the Lambda function for the S3 bucket.",
            "Configure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function.",
            "Increase the memory of the Lambda function to give the function the ability to process large files from the S3 bucket."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 26,
        "text": "A company has a single developer writing code for an automated deployment pipeline. The developer is storing source code in an Amazon S3 bucket for each project. The company wants to add more developers to the team but is concerned about code conflicts and lost work. The company also wants to build a test environment to deploy newer versions of code for testing and allow developers to automatically deploy to both environments when code is changed in the repository.What is the MOST efficient way to meet these requirements?",
        "options": [
            "Create an AWS CodeCommit repository for each project, and use the main branch for production and test code with different deployment pipelines for each environment. Use feature branches to develop new features.",
            "Create an AWS CodeCommit repository for each project, use the main branch for production code, and create a testing branch for code deployed to testing. Use feature branches to develop new features and pull requests to merge code to testing and main branches.",
            "Create another S3 bucket for each project for testing code, and use an AWS Lambda function to promote code changes between testing and production buckets. Enable versioning on all buckets to prevent code conflicts.",
            "Enable versioning and branching on each S3 bucket, use the main branch for production code, and create a testing branch for code deployed to testing. Have developers use each branch for developing in each environment."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 27,
        "text": "A company's security policies require the use of security hardened AMIs in production environments. A DevOps engineer has used EC2 Image Builder to create a pipeline that builds the AMIs on a recurring schedule.The DevOps engineer needs to update the launch templates of the company's Auto Scaling groups. The Auto Scaling groups must use the newest AMIs during the launch of Amazon EC2 instances.Which solution will meet these requirements with the MOST operational efficiency?",
        "options": [
            "Configure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Systems Manager Run Command document that updates the launch templates of the Auto Scaling groups with the newest AMI ID.",
            "Configure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Lambda function that updates the launch templates of the Auto Scaling groups with the newest AMI ID.",
            "Configure the launch template to use a value from AWS Systems Manager Parameter Store for the AMI ID. Configure the Image Builder pipeline to update the Parameter Store value with the newest AMI ID.",
            "Configure the Image Builder distribution settings to update the launch templates with the newest AMI IConfigure the Auto Scaling groups to use the newest version of the launch template."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 28,
        "text": "A company builds a container image in an AWS CodeBuild project by running Docker commands. After the container image is built, the CodeBuild project uploads the container image to an Amazon S3 bucket. The CodeBuild project has an IAM service role that has permissions to access the S3 bucket.A DevOps engineer needs to replace the S3 bucket with an Amazon Elastic Container Registry (Amazon ECR) repository to store the container images. The DevOps engineer creates an ECR private image repository in the same AWS Region of the CodeBuild project. The DevOps engineer adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository. The DevOps engineer also places new repository information into the docker build command and the docker push command that are used in the buildspec.yml file.When the CodeBuild project runs a build job, the job fails when the job tries to access the ECR repository.Which solution will resolve the issue of failed access to the ECR repository?",
        "options": [
            "Add an environment variable of type SECRETS_MANAGER to the CodeBuild project. In the environment variable, include the ARN of the CodeBuild project's IAM service role. Update the buildspec.yml file to use the new environment variable to log in with the docker login command to access the ECR repository.",
            "Update the buildspec.yml file to use the AWS CLI to assume the IAM service role for ECR operations. Add an ECR repository policy that allows the IAM service role to have access.",
            "Update the ECR repository to be a public image repository. Add an ECR repository policy that allows the IAM service role to have access.",
            "Update the buildspec.yml file to log in to the ECR repository by using the aws ecr get-login-password AWS CLI command to obtain an authentication token. Update the docker login command to use the authentication token to access the ECR repository."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 29,
        "text": "A company builds an application that uses an Application Load Balancer in front of Amazon EC2 instances that are in an Auto Scaling group. The application is stateless. The Auto Scaling group uses a custom AMI that is fully prebuilt. The EC2 instances do not have a custom bootstrapping process.The AMI that the Auto Scaling group uses was recently deleted. The Auto Scaling group's scaling activities show failures because the AMI ID does not exist.Which combination of steps should a DevOps engineer take to meet these requirements? (Choose three.)",
        "options": [
            "Create a new AMI by copying the most recent public AMI of the operating system that the EC2 instances use.",
            "Create a new AMI from a running EC2 instance in the Auto Scaling group.",
            "Create a new launch template that uses the new AMI.",
            "Update the Auto Scaling group to use the new launch template.",
            "Increase the Auto Scaling group's desired capacity by 1.",
            "Reduce the Auto Scaling group's desired capacity to 0."
        ],
        "correct": [
            1,
            2,
            3
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 30,
        "text": "A company's DevOps engineer is creating an AWS Lambda function to process notifications from an Amazon Simple Notification Service (Amazon SNS) topic. The Lambda function will process the notification messages and will write the contents of the notification messages to an Amazon RDS Multi-AZ DB instance.During testing, a database administrator accidentally shut down the DB instance. While the database was down the company lost several of the SNS notification messages that were delivered during that time.The DevOps engineer needs to prevent the loss of notification messages in the future.Which solutions will meet this requirement? (Choose two.)",
        "options": [
            "Replace the SNS topic with an Amazon EventBridge event bus. Configure an EventBridge rule on the new event bus to invoke the Lambda function for each event.",
            "Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.",
            "Replace the RDS Multi-AZ DB instance with an Amazon DynamoDB table.",
            "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination of the Lambda function.",
            "Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue."
        ],
        "correct": [
            1,
            4
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 31,
        "text": "A company has deployed a critical application in two AWS Regions. The application uses an Application Load Balancer (ALB) in both Regions. The company has Amazon Route 53 alias DNS records for both ALBs.The company uses Amazon Route 53 Application Recovery Controller to ensure that the application can fail over between the two Regions. The Route 53 ARC configuration includes a routing control for both Regions. The company uses Route 53 ARC to perform quarterly disaster recovery (DR) tests.During the most recent DR test, a DevOps engineer accidentally turned off both routing controls. The company needs to ensure that at least one routing control is turned on at all times.Which solution will meet these requirements?",
        "options": [
            "In Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53RecoveryReadiness::DNSTargetResource resource type. Add the domain names of the two Route 53 alias DNS records as the target resource. Create a new readiness check for the resource set.",
            "In Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53::HealthCheck resource type. Specify the ARNs of the two routing controls as the target resource. Create a new readiness check for the resource set.",
            "In Route 53 ARC, create a new gating safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with the OR type with a threshold of 1.",
            "In Route 53 ARC, create a new assertion safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with the ATLEAST type with a threshold of 1."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 32,
        "text": "A company is launching an application. The application must use only approved AWS services. The account that runs the application was created less than 1 year ago and is assigned to an AWS Organizations OU.The company needs to create a new Organizations account structure. The account structure must have an appropriate SCP that supports the use of only services that are currently active in the AWS account. The company will use AWS Identity and Access Management (IAM) Access Analyzer in the solution.Which solution will meet these requirements?",
        "options": [
            "Create an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the management account. Detach the default FullAWSAccess SCP from the new OU.",
            "Create an SCP that allows the services that IAM Access Analyzer identifies. Attach the new SCP to the organization's root.",
            "Create an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the new OU. Detach the default FullAWSAccess SCP from the new OU.",
            "Create an SCP that denies the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the new OU."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 33,
        "text": "A DevOps engineer is using AWS CodeDeploy across a fleet of Amazon EC2 instances in an EC2 Auto Scaling group. The associated CodeDeploy deployment group, which is integrated with EC2 Auto Scaling, is configured to perform in-place deployments with CodeDeployDefault.OneAtATime. During an ongoing new deployment, the engineer discovers that, although the overall deployment finished successfully, two out of five instances have the previous application revision deployed. The other three instances have the newest application revision.What is likely causing this issue?",
        "options": [
            "The two affected instances failed to fetch the new deployment.",
            "A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances.",
            "EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.",
            "The CodeDeploy agent was not installed in two affected instances."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 34,
        "text": "A company recently migrated its legacy application from on-premises to AWS. The application is hosted on Amazon EC2 instances behind an Application Load Balancer, which is behind Amazon API Gateway. The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application. The company also wants to ensure it can quickly roll back updates if there is an issue.Which solution will meet these requirements with MINIMAL changes to the application?",
        "options": [
            "Introduce changes as a separate environment parallel to the existing one. Configure API Gateway to use a canary release deployment to send a small subset of user traffic to the new environment.",
            "Introduce changes as a separate environment parallel to the existing one. Update the application‚Äôs DNS alias records to point to the new environment.",
            "Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route user traffic to the new target group in steps.",
            "Introduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route all traffic to the Application Load Balancer, which then sends the traffic to the new target group."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 35,
        "text": "A company hired a penetration tester to simulate an internal security breach. The tester performed port scans on the company's Amazon EC2 instances. The company's security measures did not detect the port scans.The company needs a solution that automatically provides notification when port scans are performed on EC2 instances. The company creates and subscribes to an Amazon Simple Notification Service (Amazon SNS) topic.What should the company do next to meet the requirement?",
        "options": [
            "Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected network reachability findings that indicate port scans. Connect the event to the SNS topic.",
            "Ensure that Amazon GuardDuty is enabled. Create an Amazon CloudWatch alarm for detected EC2 and port scan findings. Connect the alarm to the SNS topic.",
            "Ensure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected CVEs that cause open port vulnerabilities. Connect the event to the SNS topic.",
            "Ensure that AWS CloudTrail is enabled. Create an AWS Lambda function to analyze the CloudTrail logs for unusual amounts of traffic from an IP address range. Connect the Lambda function to the SNS topic."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 36,
        "text": "A company has an AWS Control Tower landing zone. The company's DevOps team creates a workload OU. A development OU and a production OU are nested under the workload OU. The company grants users full access to the company's AWS accounts to deploy applications.The DevOps team needs to allow only a specific management IAM role to manage the IAM roles and policies of any AWS accounts in only the production OU.Which combination of steps will meet these requirements? (Choose two.)",
        "options": [
            "Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the workload OU.",
            "Create an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the production OU.",
            "Ensure that the FullAWSAccess SCP is applied at the organization root.",
            "Create an SCP that denies full access with a condition to exclude the management IAM role for the organization root.",
            "Create an SCP that allows IAM related actions. Attach the SCP to the development OU."
        ],
        "correct": [
            1,
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 37,
        "text": "The security team depends on AWS CloudTrail to detect sensitive security issues in the company‚Äôs AWS account. The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off in an AWS account.What solution ensures the LEAST amount of downtime for the CloudTrail log deliveries?",
        "options": [
            "Create an Amazon EventBridge rule for a scheduled event every 5 minutes. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on a CloudTrail trail in the AWS account. Add the Lambda function ARN as a target to the EventBridge rule.",
            "Launch a t2.nano instance with a script running every 5 minutes that uses the AWS SDK to query CloudTrail in the current account. If the CloudTrail trail is disabled, have the script re-enable the trail.",
            "Deploy the AWS-managed CloudTrail-enabled AWS Config rule, set with a periodic interval of 1 hour. Create an Amazon EventBridge rule for AWS Config rules compliance change. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule.",
            "Create an Amazon EventBridge rule for the CloudTrail StopLogging event. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 38,
        "text": "A company manages 500 AWS accounts that are in an organization in AWS Organizations. The company discovers many unattached Amazon Elastic Block Store (Amazon EBS) volumes in all the accounts. The company wants to automatically tag the unattached EBS volumes for investigation.A DevOps engineer needs to deploy an AWS Lambda function to all the AWS accounts. The Lambda function must run every 30 minutes to tag all the EBS volumes that have been unattached for a period of 7 days or more.Which solution will meet these requirements in the MOST operationally efficient manner?",
        "options": [
            "Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the Lambda function in each member account every 30 minutes.",
            "Create a cross-account IAM role in the organization's member accounts. Attach the AWSLambda_FullAccess policy and the AWSCloudFormationFullAccess policy to the role. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Create a custom script in the organization‚Äôs management account that assumes the role and deploys the CloudFormation template to the member accounts.",
            "Create a cross-account IAM role in the organization's member accounts. Attach the AmazonS3FullAccess policy and the AWSCodeDeployDeployerAccess policy to the role. Use AWS CodeDeploy to assume the role to deploy the Lambda function from the organization's management account. Configure an Amazon EventBridge scheduled rule in the member accounts to invoke the Lambda function every 30 minutes.",
            "Configure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization"
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 39,
        "text": "A company has a data ingestion application that runs across multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to monitor the application and consolidate access to the application. Currently, the company is running the application on Amazon EC2 instances from several Auto Scaling groups. The EC2 instances have no access to the internet because the data is sensitive. Engineers have deployed the necessary VPC endpoints. The EC2 instances run a custom AMI that is built specifically for the application.To maintain and troubleshoot the application, system administrators need the ability to log in to the EC2 instances. This access must be automated and controlled centrally. The company‚Äôs security team must receive a notification whenever the instances are accessed.Which solution will meet these requirements?",
        "options": [
            "Use EC2 Image Builder to rebuild the custom AMI. Include the most recent version of AWS Systems Manager Agent in the image. Configure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.",
            "Create an Amazon EventBridge rule to send notifications to the security team whenever a user logs in to an EC2 instance. Use EC2 Instance Connect to log in to the instances. Deploy Auto Scaling groups by using AWS CloudFormation. Use the cfn-init helper script to deploy appropriate VPC routes for external access. Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.",
            "Use AWS Systems Manager Automation to build Systems Manager Agent into the custom AMI. Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.",
            "Deploy a NAT gateway and a bastion host that has internet access. Create a security group that allows incoming traffic on all the EC2 instances from the bastion host. Install AWS Systems Manager Agent on all the EC2 instances. Use Auto Scaling group lifecycle hooks for monitoring and auditing access. Use Systems Manager Session Manager to log in to the instances. Send logs to a log group in Amazon CloudWatch Logs. Export data to Amazon S3 for auditing. Send notifications to the security team by using S3 event notifications."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 40,
        "text": "A DevOps engineer has developed an AWS Lambda function. The Lambda function starts an AWS CloudFormation drift detection operation on all supported resources for a specific CloudFormation stack. The Lambda function then exits its invocation.The DevOps engineer has created an Amazon EventBridge scheduled rule that invokes the Lambda function every hour. An Amazon Simple Notification Service (Amazon SNS) topic already exists in the AWS account. The DevOps engineer has subscribed to the SNS topic to receive notifications.The DevOps engineer needs to receive a notification as soon as possible when drift is detected in this specific stack configuration.Which solution will meet these requirements?",
        "options": [
            "Configure the existing EventBridge rule to also target the SNS topic. Configure an SNS subscription filter policy to match the CloudFormation stack. Attach the subscription filter policy to the SNS topic.",
            "Configure AWS Config in the account. Use the cloudformation-stack-drift-detection-check managed rule. Create a second EventBridge rule that reacts to a compliance change event for the CloudFormation stack. Configure the SNS topic as a target of the second EventBridge rule.",
            "Create a second Lambda function to query the CloudFormation API for the drift detection results for the stack. Configure the second Lambda function to publish a message to the SNS topic if drift is detected. Adjust the existing EventBridge rule to also target the second Lambda function.",
            "Configure Amazon GuardDuty in the account with drift detection for all CloudFormation stacks. Create a second EventBridge rule that reacts to the GuardDuty drift detection event finding for the specific CloudFormation stack. Configure the SNS topic as a target of the second EventBridge rule."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 41,
        "text": "A healthcare services company is concerned about the growing costs of software licensing for an application for monitoring patient wellness. The company wants to create an audit process to ensure that the application is running exclusively on Amazon EC2 Dedicated Hosts. A DevOps engineer must create a workflow to audit the application to ensure compliance.What steps should the engineer take to meet this requirement with the LEAST administrative overhead?",
        "options": [
            "Use AWS Systems Manager Configuration Compliance. Use calls to the put-compliance-items API action to scan and build a database of noncompliant EC2 instances based on their host placement configuration. Use an Amazon DynamoDB table to store these instance IDs for fast access. Generate a report through Systems Manager by calling the list-compliance-summaries API action.",
            "Use AWS CloudTrail. Identify all EC2 instances to be audited by analyzing all calls to the EC2 RunCommand API action. Invoke an AWS Lambda function that analyzes the host placement of the instance. Store the EC2 instance ID of noncompliant resources in an Amazon RDS for MySQL DB instance. Generate a report by querying the RDS instance and exporting the query results to a CSV text file.",
            "Use AWS Config. Identify all EC2 instances to be audited by enabling Config Recording on all Amazon EC2 resources for the region. Create a custom AWS Config rule that triggers an AWS Lambda function by using the \"config-rule-change -triggered\" blueprint. Modify the Lambda evaluateCompliance() function to verify host placement to return a NON_COMPLIANT result if the instance is not running on an EC2 Dedicated Host. Use the AWS Config report to address noncompliant instances.",
            "Use custom Java code running on an EC2 instance. Set up EC2 Auto Scaling for the instance depending on the number of instances to be checked. Send the list of noncompliant EC2 instance IDs to an Amazon SQS queue. Set up another worker instance to process instance IDs from the SQS queue and write them to Amazon DynamoDUse an AWS Lambda function to terminate noncompliant instance IDs obtained from the queue, and send them to an Amazon SNS email topic for distribution."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 42,
        "text": "A company sells products through an ecommerce web application. The company wants a dashboard that shows a pie chart of product transaction details. The company wants to integrate the dashboard with the company's existing Amazon CloudWatch dashboards.Which solution will meet these requirements with the MOST operational efficiency?",
        "options": [
            "Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Use CloudWatch Logs Insights to query the log group and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.",
            "Update the ecommerce application to use AWS X-Ray for instrumentation. Create a new X-Ray subsegment. Add an annotation for each processed transaction. Use X-Ray traces to query the data and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.",
            "Update the ecommerce application to emit a JSON object to an Amazon S3 bucket for each processed transaction. Use Amazon Athena to query the S3 bucket and to visualize the results in a pie chart format. Export the results from Athena. Attach the results to the desired CloudWatch dashboard.",
            "Update the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Create an AWS Lambda function to aggregate and write the results to Amazon DynamoDB. Create a Lambda subscription filter for the log file. Attach the results to the desired CloudWatch dashboard."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 43,
        "text": "A company has an application that runs on Amazon EC2 instances. The company uses an AWS CodePipeline pipeline to deploy the application into multiple AWS Regions. The pipeline is configured with a stage for each Region. Each stage contains an AWS CloudFormation action for each Region.When the pipeline deploys the application to a Region, the company wants to confirm that the application is in a healthy state before the pipeline moves on to the next Region. Amazon Route 53 record sets are configured for the application in each Region. A DevOps engineer creates a Route 53 health check that is based on an Amazon CloudWatch alarm for each Region where the application is deployed.What should the DevOps engineer do next to meet the requirements?",
        "options": [
            "Configure an AWS CodeDeploy application to deploy a CloudFormation template with automatic rollback. Configure the CloudWatch alarm as the instance health check for the CodeDeploy application. Remove the CloudFormation actions from the pipeline. Create a CodeDeploy action in the pipeline stage for each Region.",
            "Create an AWS Step Functions workflow to check the state of the CloudWatch alarm. Configure the Step Functions workflow to exit with an error if the alarm is in the ALARM state. Create a new stage in the pipeline between each Region deployment stage. In each new stage, include an action to invoke the Step Functions workflow.",
            "Configure the CloudWatch agent on the EC2 instances to report the application status to the Route 53 health check. Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action to exit with an error if the CloudWatch alarm is in the ALARM state.",
            "Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action for the new stage to check the state of the CloudWatch alarm and to exit with an error if the alarm is in the ALARM state"
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 44,
        "text": "A company's application runs on Amazon EC2 instances. The application writes to a log file that records the username, date, time, and source IP address of the login. The log is published to a log group in Amazon CloudWatch Logs.The company is performing a root cause analysis for an event that occurred on the previous day. The company needs to know the number of logins for a specific user from the past 7 days.Which solution will provide this information?",
        "options": [
            "Create a CloudWatch Logs metric filter on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.",
            "Create a CloudWatch Logs subscription on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.",
            "Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group.",
            "Create a CloudWatch dashboard. Add a number widget that has a filter pattern that counts the number of logins for the username over the past 7 days directly from the log group."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 45,
        "text": "A company uses AWS Control Tower and AWS CloudFormation to manage its AWS accounts and to create AWS resources. The company requires all Amazon S3 buckets to be encrypted with AWS Key Management Service (AWS KMS) when the S3 buckets are created in a CloudFormation stack.Which solution will meet this requirement?",
        "options": [
            "Use AWS Organizations. Attach an SCP that denies the s3:PutObject permission if the request does not include an x-amz-server-side-encryption header that requests server-side encryption with AWS KMS keys (SSE-KMS).",
            "Use AWS Control Tower with a multi-account environment. Configure and enable detective AWS Control Tower controls on all OUs with CloudFormation hooks.",
            "Use AWS Control Tower with a multi-account environment. Configure and enable proactive AWS Control Tower controls on all OUs with CloudFormation hooks.",
            "Use AWS Organizations. Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. Deploy the rule. Create and apply an SCP to prevent users from stopping and deleting AWS Config across all AWS accounts,"
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 46,
        "text": "A DevOps engineer is setting up a container-based architecture. The engineer has decided to use AWS CloudFormation to automatically provision an Amazon ECS cluster and an Amazon EC2 Auto Scaling group to launch the EC2 container instances. After successfully creating the CloudFormation stack, the engineer noticed that, even though the ECS cluster and the EC2 instances were created successfully and the stack finished the creation, the EC2 instances were associating with a different cluster.How should the DevOps engineer update the CloudFormation template to resolve this issue?",
        "options": [
            "Reference the ECS cluster in the AWS::EC2::Instance resource of the UserData property.",
            "Reference the EC2 instances in the AWS::ECS::Cluster resource and reference the ECS cluster in the AWS::ECS::Service resource.",
            "Reference the ECS cluster in the AWS::AutoScaling::LaunchConfiguration resource of the UserData property.",
            "Reference the ECS cluster in the AWS::CloudFormation::CustomResource resource to trigger an AWS Lambda function that registers the EC2 instances with the appropriate ECS cluster."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 47,
        "text": "A DevOps team uses AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy to deploy an application. The application is a REST API that uses AWS Lambda functions and Amazon API Gateway. Recent deployments have introduced errors that have affected many customers.The DevOps team needs a solution that reverts to the most recent stable version of the application when an error is detected. The solution must affect the fewest customers possible.Which solution will meet these requirements with the MOST operational efficiency?",
        "options": [
            "Set the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.",
            "Set the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.",
            "Set the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure manual rollbacks on the deployment group. Create an Amazon Simple Notification Service (Amazon SNS) topic to send notifications every time a deployment fails. Configure the SNS topic to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment.",
            "Set the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure manual rollbacks on the deployment group. Create a metric filter on an Amazon CloudWatch log group for API Gateway to monitor HTTP Bad Gateway errors. Configure the metric filter to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 48,
        "text": "A company uses AWS Organizations to manage its AWS accounts. The company has a root OU that has a child OU. The root OU has an SCP that allows all actions on all resources. The child OU has an SCP that allows all actions for Amazon DynamoDB and AWS Lambda, and denies all other actions.The company has an AWS account that is named vendor-data in the child OU. A DevOps engineer has an IAM user that is attached to the Administrator Access IAM policy in the vendor-data account. The DevOps engineer attempts to launch an Amazon EC2 instance in the vendor-data account but receives an access denied error.Which change should the DevOps engineer make to launch the EC2 instance in the vendor-data account?",
        "options": [
            "Update the SCP in the child OU to allow all actions for Amazon EC2.",
            "Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the root OU.",
            "Create a new SCP that allows all actions for Amazon EC2. Attach the SCP to the vendor-data account.",
            "Attach the AmazonEC2FullAccess IAM policy to the IAM user."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 49,
        "text": "A DevOps engineer is designing an application that integrates with a legacy REST API. The application has an AWS Lambda function that reads records from an Amazon Kinesis data stream. The Lambda function sends the records to the legacy REST API.Approximately 10% of the records that the Lambda function sends from the Kinesis data stream have data errors and must be processed manually. The Lambda function event source configuration has an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as an on-failure destination. The DevOps engineer has configured the Lambda function to process records in batches and has implemented retries in case of failure.During testing, the DevOps engineer notices that the dead-letter queue contains many records that have no data errors and that already have been processed by the legacy REST API. The DevOps engineer needs to configure the Lambda function's event source options to reduce the number of errorless records that are sent to the dead-letter queue.Which solution will meet these requirements?",
        "options": [
            "Decrease the maximum age of record.",
            "Increase the concurrent batches per shard.",
            "Configure the setting to split the batch when an error occurs.",
            "Increase the retry attempts."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 50,
        "text": "A company deploys its corporate infrastructure on AWS across multiple AWS Regions and Availability Zones. The infrastructure is deployed on Amazon EC2 instances and connects with AWS IoT Greengrass devices. The company deploys additional resources on on-premises servers that are located in the corporate headquarters.The company wants to reduce the overhead involved in maintaining and updating its resources. The company‚Äôs DevOps team plans to use AWS Systems Manager to implement automated management and application of patches. The DevOps team confirms that Systems Manager is available in the Regions that the resources are deployed in. Systems Manager also is available in a Region near the corporate headquarters.Which combination of steps must the DevOps team take to implement automated patch and configuration management across the company‚Äôs EC2 instances, IoT devices, and on-premises infrastructure? (Choose three.)",
        "options": [
            "Generate a managed-instance activation. Use the Activation Code and Activation ID to install Systems Manager Agent (SSM Agent) on each server in the on-premises environment. Update the AWS IoT Greengrass IAM token exchange role. Use the role to deploy SSM Agent on all the IoT devices.",
            "Use Systems Manager Patch Manager to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers as a Systems Manager maintenance window task.",
            "Apply tags to all the EC2 instances, AWS IoT Greengrass devices, and on-premises servers. Use Systems Manager Session Manager to push patches to all the tagged devices.",
            "Use Systems Manager Run Command to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers.",
            "Create an IAM instance profile for Systems Manager. Attach the instance profile to all the EC2 instances in the AWS account. For the AWS IoT Greengrass devices and on-premises servers, create an IAM service role for Systems Manager.",
            "Configure Amazon EventBridge to monitor Systems Manager Patch Manager for updates to patch baselines. Associate Systems Manager Run Command with the event to initiate a patch action for all EC2 instances, AWS IoT Greengrass devices, and on-premises servers."
        ],
        "correct": [
            0,
            1,
            4
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 51,
        "text": "A DevOps engineer is working on a project that is hosted on Amazon Linux and has failed a security review. The DevOps manager has been asked to review the company buildspec.yaml file for an AWS CodeBuild project and provide recommendations. The buildspec.yaml file is configured as follows:What changes should be recommended to comply with AWS security best practices? (Choose three.)",
        "options": [
            "Update the CodeBuild project role with the necessary permissions and then remove the AWS credentials from the environment variable.",
            "Use AWS Systems Manager run command versus scp and ssh commands directly to the instance.",
            "Store the DB_PASSWORD as a SecureString value in AWS Systems Manager Parameter Store and then remove the DB_PASSWORD from the environment variables.",
            "Move the environment variables to the ‚Äòdb-deploy-bucket‚Äô Amazon S3 bucket, add a prebuild stage to download, then export the variables.",
            "Add a post-build command to remove the temporary files from the container before termination to ensure they cannot be seen by other CodeBuild users.",
            "Scramble the environment variables using XOR followed by Base64, add a section to install, and then run XOR and Base64 to the build phase."
        ],
        "correct": [
            0,
            1,
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 52,
        "text": "A global company manages multiple AWS accounts by using AWS Control Tower. The company hosts internal applications and public applications.Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations. One of the AWS Control Tower member accounts serves as a centralized DevOps account with CI/CD pipelines that application teams use to deploy applications to their respective target AWS accounts. An IAM role for deployment exists in the centralized DevOps account.An application team is attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in an application AWS account. An IAM role for deployment exists in the application AWS account. The deployment is through an AWS CodeBuild project that is set up in the centralized DevOps account. The CodeBuild project uses an IAM service role for CodeBuild. The deployment is failing with an Unauthorized error during attempts to connect to the cross-account EKS cluster from CodeBuild.Which solution will resolve this error?",
        "options": [
            "Configure the centralized DevOps account‚Äôs deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the centralized DevOps account‚Äôs deployment IAM role to allow the required access to CodeBuild.",
            "Configure the centralized DevOps account‚Äôs deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRoleWithSAML action. Configure the centralized DevOps account‚Äôs deployment IAM role to allow the required access to CodeBuild.",
            "Configure the application account‚Äôs deployment IAM role to have a trust relationship with the AWS Control Tower management account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account‚Äôs deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.",
            "Configure the application account‚Äôs deployment IAM role to have a trust relationship with the centralized DevOps account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account‚Äôs deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 53,
        "text": "A company is developing an application that will generate log events. The log events consist of five distinct metrics every one tenth of a second and produce a large amount of data.The company needs to configure the application to write the logs to Amazon Timestream. The company will configure a daily query against the Timestream table.Which combination of steps will meet these requirements with the FASTEST query performance? (Choose three.)",
        "options": [
            "Treat each log as a multi-measure record.",
            "Treat each log as a single-measure record.",
            "Use batch writes to write multiple log events in a single write operation.",
            "Write each log event as a single write operation.",
            "Configure the memory store retention period to be shorter than the magnetic store retention period.",
            "Configure the memory store retention period to be longer than the magnetic store retention period."
        ],
        "correct": [
            0,
            2,
            4
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 54,
        "text": "A company uses AWS Organizations to manage its AWS accounts. The organization root has an OU that is named Environments. The Environments OU has two child OUs that are named Development and Production, respectively.The Environments OU and the child OUs have the default FullAWSAccess policy in place. A DevOps engineer plans to remove the FullAWSAccess policy from the Development OU and replace the policy with a policy that allows all actions on Amazon EC2 resources.What will be the outcome of this policy replacement?",
        "options": [
            "All users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.",
            "All users in the Development OU will be denied all API actions on all resources.",
            "All users in the Development OU will be denied all API actions on EC2 resources. All other API actions will be allowed.",
            "All users in the Development OU will be allowed all API actions on all resources."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 55,
        "text": "A company uses Amazon S3 to store proprietary information. The development team creates buckets for new projects on a daily basis. The security team wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled. Additionally, no buckets should ever be publicly read or write accessible.What should a DevOps engineer do to meet these requirements?",
        "options": [
            "Enable AWS CloudTrail and configure automatic remediation using AWS Lambda.",
            "Enable AWS Trusted Advisor and configure automatic remediation using Amazon EventBridge.",
            "Enable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.",
            "Enable AWS Systems Manager and configure automatic remediation using Systems Manager documents."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 56,
        "text": "A company has a mission-critical application on AWS that uses automatic scaling. The company wants the deployment lifecycle to meet the following parameters:‚Ä¢ The application must be deployed one instance at a time to ensure the remaining fleet continues to serve traffic.‚Ä¢ The application is CPU intensive and must be closely monitored.‚Ä¢ The deployment must automatically roll back if the CPU utilization of the deployment instance exceeds 85%.Which solution will meet these requirements?",
        "options": [
            "Use AWS CodeDeploy with Amazon EC2 Auto Scaling Configure an alarm tied to the CPU utilization metric. Use the CodeDeployDefault OneAtAtime configuration as a deployment strategy. Configure automatic rollbacks within the deployment group to roll back the deployment if the alarm thresholds are breached.",
            "Use AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move to one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group using the heartbeat timeout.",
            "Use AWS Elastic Beanstalk for load balancing and AWS Auto Scaling. Configure an alarm tied to the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Enable enhanced health to monitor the status of the deployment and roll back based on the alarm previously created.",
            "Use AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks within the Auto Scaling group to roll back the deployment if the alarm thresholds are breached."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 57,
        "text": "A company has an application and a CI/CD pipeline. The CI/CD pipeline consists of an AWS CodePipeline pipeline and an AWS CodeBuild project. The CodeBuild project runs tests against the application as part of the build process and outputs a test report. The company must keep the test reports for 90 days.Which solution will meet these requirements?",
        "options": [
            "Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure an Amazon EventBridge rule that invokes an AWS Lambda function to copy the reports to the S3 bucket when a build is completed. Create an S3 Lifecycle rule to expire the objects after 90 days.",
            "Add a new stage in the CodePipeline pipeline. Configure a test action type with the appropriate path and format for the reports. Configure the report expiration time to be 90 days in the CodeBuild project buildspec file.",
            "Add a new stage in the CodePipeline pipeline after the stage that contains the CodeBuild project. Create an Amazon S3 bucket to store the reports. Configure an S3 deploy action type in the new CodePipeline stage with the appropriate path and format for the reports.",
            "Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure the report group as an artifact in the CodeBuild project buildspec file. Configure the S3 bucket as the artifact destination. Set the object expiration to 90 days."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 58,
        "text": "A company's application teams use AWS CodeCommit repositories for their applications. The application teams have repositories in multiple AWS accounts. All accounts are in an organization in AWS Organizations.Each application team uses AWS IAM Identity Center (AWS Single Sign-On) configured with an external IdP to assume a developer IAM role. The developer role allows the application teams to use Git to work with the code in the repositories.A security audit reveals that the application teams can modify the main branch in any repository. A DevOps engineer must implement a solution that allows the application teams to modify the main branch of only the repositories that they manage.Which combination of steps will meet these requirements? (Choose three.)",
        "options": [
            "Create an approval rule template for each team in the Organizations management account. Associate the template with all the repositories. Add the developer role ARN as an approver.",
            "Update the SAML assertion to pass the user's team name. Update the IAM role's trust policy to add an access-team session tag that has the team name.",
            "Attach an SCP to the accounts. Include the following statement:",
            "Create an IAM permissions boundary in each account. Include the following statement:",
            "For each CodeCommit repository, add an access-team tag that has the value set to the name of the associated team.",
            "Create an approval rule template for each account. Associate the template with all repositories. Add the \"aws:ResourceTag/access-team\": \"$ ;{aws:PrincipalTag/access-team}\" condition to the approval rule template."
        ],
        "correct": [
            1,
            2,
            4
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 59,
        "text": "A company is using AWS CodePipeline to deploy an application. According to a new guideline, a member of the company's security team must sign off on any application changes before the changes are deployed into production. The approval must be recorded and retained.Which combination of actions will meet these requirements? (Choose two.)",
        "options": [
            "Configure CodePipeline to write actions to an Amazon S3 bucket at the end of each pipeline stage.",
            "Create a CodePipeline manual approval action before the deployment step. Create a policy that grants the security team access to approve manual approval stages.",
            "Create an AWS CloudTrail trail to deliver logs to Amazon S3.",
            "Create a CodePipeline custom action to invoke an AWS Lambda function for approval. Create a policy that gives the security team access to manage CodePipeline custom actions.",
            "Configure CodePipeline to write actions to Amazon CloudWatch Logs."
        ],
        "correct": [
            1,
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 60,
        "text": "A DevOps engineer is researching the least expensive way to implement an image batch processing cluster on AWS. The application cannot run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS volume and can tolerate interruptions. Configuring the cluster software from a generic EC2 Linux image takes 30 minutes.What is the MOST cost-effective solution?",
        "options": [
            "Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and utilize user data to configure the EC2 Linux instance on startup.",
            "Use Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision EC2 instances temporarily.",
            "Use GlusterFS on EC2 instances for checkpoint data. To run the batch job, configure EC2 instances manually. When the job completes, shut down the instances manually.",
            "Use Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a custom AMI for the cluster and use the latest AMI when creating instances."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 61,
        "text": "A company uses an Amazon API Gateway regional REST API to host its application API. The REST API has a custom domain. The REST API's default endpoint is deactivated.The company's internal teams consume the API. The company wants to use mutual TLS between the API and the internal teams as an additional layer of authentication.Which combination of steps will meet these requirements? (Choose two.)",
        "options": [
            "Upload the provisioned client certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the client certificate that is stored in the S3 bucket as the trust store.",
            "Use AWS Certificate Manager (ACM) to create a private certificate authority (CA). Provision a client certificate that is signed by the private CA.",
            "Upload the root private certificate authority (CA) certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the private CA certificate that is stored in the S3 bucket as the trust store.",
            "Upload the provisioned client certificate private key to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the private key that is stored in the S3 bucket as the trust store.",
            "Provision a client certificate that is signed by a public certificate authority (CA). Import the certificate into AWS Certificate Manager (ACM)."
        ],
        "correct": [
            1,
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 62,
        "text": "A company is migrating from its on-premises data center to AWS. The company currently uses a custom on-premises Cl/CD pipeline solution to build and package software.The company wants its software packages and dependent public repositories to be available in AWS CodeArtifact to facilitate the creation of application-specific pipelines.Which combination of steps should the company take to update the CI/CD pipeline solution and to configure CodeArtifact with the LEAST operational overhead? (Choose two.)",
        "options": [
            "Create an AWS Identity and Access Management Roles Anywhere trust anchor. Create an IAM role that allows CodeArtifact actions and that has a trust relationship on the trust anchor. Update the on-premises CI/CD pipeline to assume the new IAM role and to publish the packages to CodeArtifact.",
            "For each public repository, create a CodeArutact repository that is configured with an external connection. Configure the dependent repositories as upstream public repositories.",
            "Create a new Amazon S3 bucket. Generate a presigned URL that allows the PutObject request. Update the on-premises CI/CD pipeline to use the presigned URL to publish the packages from the on-premises location to the S3 bucket. Create an AWS Lambda function that runs when packages are created in the bucket through a put command. Configure the Lambda function to publish the packages to CodeArtifact.",
            "Update the C1ICD pipeline to create a VM image that contains newly packaged software. Use AWS Import/Export to make the VM image available as an Amazon EC2 AMI. Launch the AMI with an attached IAM instance profile that allows CodeArtifact actions. Use AWS CLI commands to publish the packages to a CodeArtifact repository.",
            "Create a Codeartitact repository that is configured with a set of external connections to the public repositories. Configure the external connections to be downstream of the repository."
        ],
        "correct": [
            0,
            1
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 63,
        "text": "A company deploys a web application on Amazon EC2 instances that are behind an Application Load Balancer (ALB). The company stores the application code in an AWS CodeCommit repository. When code is merged to the main branch, an AWS Lambda function invokes an AWS CodeBuild project. The CodeBuild project packages the code, stores the packaged code in AWS CodeArtifact, and invokes AWS Systems Manager Run Command to deploy the packaged code to the EC2 instances.Previous deployments have resulted in defects, EC2 instances that are not running the latest version of the packaged code, and inconsistencies between instances.Which combination of actions should a DevOps engineer take to implement a more reliable deployment solution? (Choose two.)",
        "options": [
            "Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Configure pipeline stages that run the CodeBuild project in parallel to build and test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.",
            "Create an AWS CodeDeploy application and a deployment group to deploy the packaged code to the EC2 instances. Configure the ALB for the deployment group.",
            "Create a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Create separate pipeline stages that run a CodeBuild project to build and then test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.",
            "Create individual Lambda functions that use AWS CodeDeploy instead of Systems Manager to run build, test, and deploy actions.",
            "Create an Amazon S3 bucket. Modify the CodeBuild project to store the packages in the S3 bucket instead of in CodeArtifact. Use deploy actions in CodeDeploy to deploy the artifact to the EC2 instances."
        ],
        "correct": [
            1,
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 64,
        "text": "A software team is using AWS CodePipeline to automate its Java application release pipeline. The pipeline consists of a source stage, then a build stage, and then a deploy stage. Each stage contains a single action that has a runOrder value of 1.The team wants to integrate unit tests into the existing release pipeline. The team needs a solution that deploys only the code changes that pass all unit tests.Which solution will meet these requirements?",
        "options": [
            "Modify the deploy stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.",
            "Modify the deploy stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.",
            "Modify the build stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.",
            "Modify the build stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 65,
        "text": "A company is using an organization in AWS Organizations to manage multiple AWS accounts. The company‚Äôs development team wants to use AWS Lambda functions to meet resiliency requirements and is rewriting all applications to work with Lambda functions that are deployed in a VPC. The development team is using Amazon Elastic File System (Amazon EFS) as shared storage in Account A in the organization.The company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B.A DevOps engineer needs to reconfigure an existing EFS file system to allow Lambda functions to access the data through an existing EFS access point.Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)",
        "options": [
            "Update the Lambda execution roles with permission to access the VPC and the EFS file system.",
            "Create a new EFS file system in Account B. Use AWS Database Migration Service (AWS DMS) to keep data from Account A and Account B synchronized.",
            "Create SCPs to set permission guardrails with fine-grained control for Amazon EFS.",
            "Update the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A.",
            "Create a VPC peering connection to connect Account A to Account B.",
            "Configure the Lambda functions in Account B to assume an existing IAM role in Account A."
        ],
        "correct": [
            0,
            3,
            4
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 66,
        "text": "A company uses AWS and has a VPC that contains critical compute infrastructure with predictable traffic patterns. The company has configured VPC flow logs that are published to a log group in Amazon CloudWatch Logs.The company's DevOps team needs to configure a monitoring solution for the VPC flow logs to identify anomalies in network traffic to the VPC over time. If the monitoring solution detects an anomaly, the company needs the ability to initiate a response to the anomaly.How should the DevOps team configure the monitoring solution to meet these requirements?",
        "options": [
            "Create an Amazon Kinesis Data Firehose delivery stream that delivers events to an Amazon S3 bucket. Subscribe the log group to the delivery stream. Configure Amazon Lookout for Metrics to monitor the data in the S3 bucket for anomalies. Create an AWS Lambda function to run in response to Lookout for Metrics anomaly findings. Configure the Lambda function to publish to the default Amazon EventBridge event bus.",
            "Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Configure Amazon Kinesis Data Analytics to detect log anomalies in the data stream. Create an AWS Lambda function to use as the output of the data stream. Configure the Lambda function to write to the default Amazon EventBridge event bus in the event of an anomaly finding.",
            "Create an Amazon Kinesis data stream. Subscribe the log group to the data stream. Create an AWS Lambda function to detect log anomalies. Configure the Lambda function to write to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Set the Lambda function as the processor for the data stream.",
            "Create an AWS Lambda function to detect anomalies. Configure the Lambda function to publish an event to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Subscribe the Lambda function to the log group."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 67,
        "text": "A company uses AWS CodeArtifact to centrally store Python packages. The CodeArtifact repository is configured with the following repository policy:A development team is building a new project in an account that is in an organization in AWS Organizations. The development team wants to use a Python library that has already been stored in the CodeArtifact repository in the organization. The development team uses AWS CodePipeline and AWS CodeBuild to build the new application. The CodeBuild job that the development team uses to build the application is configured to run in a VPC. Because of compliance requirements, the VPC has no internet connectivity.The development team creates the VPC endpoints for CodeArtifact and updates the CodeBuild buildspec.yaml file. However, the development team cannot download the Python library from the repository.Which combination of steps should a DevOps engineer take so that the development team can use CodeArtifact? (Choose two.)",
        "options": [
            "Update the role that the CodeBuild project uses so that the role has sufficient permissions to use the CodeArtifact repository.",
            "Create an Amazon S3 gateway endpoint. Update the route tables for the subnets that are running the CodeBuild job.",
            "Share the CodeArtifact repository with the organization by using AWS Resource Access Manager (AWS RAM).",
            "Specify the account that hosts the repository as the delegated administrator for CodeArtifact in the organization.",
            "Update the repository policy‚Äôs Principal statement to include the ARN of the role that the CodeBuild project uses."
        ],
        "correct": [
            0,
            1
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 68,
        "text": "A company has a new AWS account that teams will use to deploy various applications. The teams will create many Amazon S3 buckets for application-specific purposes and to store AWS CloudTrail logs. The company has enabled Amazon Macie for the account.A DevOps engineer needs to optimize the Macie costs for the account without compromising the account's functionality.Which solutions will meet these requirements? (Choose two.)",
        "options": [
            "Configure discovery jobs to include S3 objects that are tagged as production only.",
            "Exclude S3 buckets that contain CloudTrail logs from automated discovery.",
            "Configure scheduled daily discovery jobs for all S3 buckets in the account.",
            "Exclude S3 buckets that have public read access from automated discovery.",
            "Configure discovery jobs to include S3 objects based on the last modified criterion."
        ],
        "correct": [
            1,
            4
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 69,
        "text": "A company runs a web application that extends across multiple Availability Zones. The company uses an Application Load Balancer (ALB) for routing, AWS Fargate for the application, and Amazon Aurora for the application data. The company uses AWS CloudFormation templates to deploy the application. The company stores all Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository in the same AWS account and AWS Region.A DevOps engineer needs to establish a disaster recovery (DR) process in another Region. The solution must meet an RPO of 8 hours and an RTO of 2 hours. The company sometimes needs more than 2 hours to build the Docker images from the Dockerfile.Which solution will meet the RTO and RPO requirements MOST cost-effectively?",
        "options": [
            "Copy the CloudFormation templates and the Dockerfile to an Amazon S3 bucket in the DR Region. Use AWS Backup to configure automated Aurora cross-Region hourly snapshots. In case of DR, build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region. Use the CloudFormation template that has the most recent Aurora snapshot and the Docker image from the ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.",
            "Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Configure Aurora automated backup Cross-Region Replication. Configure ECR Cross-Region Replication. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.",
            "Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Deploy a second application CloudFormation stack in the DR Region. Reconfigure Aurora to be a global database. Update both CloudFormation stacks when a new application release in the current Region is needed. In case of DR, update the application DNS records to point to the new ALB.",
            "Copy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Use Amazon EventBridge to schedule an AWS Lambda function to take an hourly snapshot of the Aurora database and of the most recent Docker image in the ECR repository. Copy the snapshot and the Docker image to the DR Region. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 70,
        "text": "A DevOps engineer used an AWS CloudFormation custom resource to set up AD Connector. The AWS Lambda function ran and created AD Connector, but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE.Which action should the engineer take to resolve this issue?",
        "options": [
            "Ensure the Lambda function IAM role has ds:ConnectDirectory permissions for the AWS account.",
            "Ensure the Lambda function code returns a response to the pre-signed URL.",
            "Ensure the Lambda function IAM role has cloudformation:UpdateStack permissions for the stack ARN.",
            "Ensure the Lambda function code has exited successfully."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 71,
        "text": "A security team is concerned that a developer can unintentionally attach an Elastic IP address to an Amazon EC2 instance in production. No developer should be allowed to attach an Elastic IP address to an instance. The security team must be notified if any production server has an Elastic IP address at any time.How can this task be automated?",
        "options": [
            "Ensure that all IAM groups associated with developers do not have associate-address permissions. Create a scheduled AWS Lambda function to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team if an instance has an Elastic IP address associated with it.",
            "Create an AWS Config rule to check that all production instances have EC2 IAM roles that include deny associate-address permissions. Verify whether there is an Elastic IP address associated with any instance, and alert the security team if an instance has an Elastic IP address associated with it.",
            "Use Amazon Athena to query AWS CloudTrail logs to check for any associate-address attempts. Create an AWS Lambda function to disassociate the Elastic IP address from the instance, and alert the security team.",
            "Attach an IAM policy to the developers' IAM group to deny associate-address permissions. Create a custom AWS Config rule to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 72,
        "text": "A DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States. The engineer must restrict which AWS Regions can be used, and ensure an alert is sent as soon as possible if any activity outside the governance policy takes place. The controls should be automatically enabled on any new Region outside the United States (US).Which combination of actions will meet these requirements? (Choose two.)",
        "options": [
            "Use an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions and send alerts if any activity is found.",
            "Create an AWS Organizations SCP that denies access to all non-global services in non-US Regions. Attach the policy to the root of the organization.",
            "Configure AWS CloudTrail to send logs to Amazon CloudWatch Logs and enable it for all Regions. Use a CloudWatch Logs metric filter to send an alert on any service activity in non-US Regions.",
            "Write an SCP using the aws:RequestedRegion condition key limiting access to US Regions. Apply the policy to all users, groups, and roles.",
            "Use an AWS Lambda function that checks for AWS service activity and deploy it to all Regions. Write an Amazon EventBridge rule that runs the Lambda function every hour, sending an alert if activity is found in a non-US Region."
        ],
        "correct": [
            1,
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 73,
        "text": "A company uses AWS Secrets Manager to store a set of sensitive API keys that an AWS Lambda function uses. When the Lambda function is invoked the Lambda function retrieves the API keys and makes an API call to an external service. The Secrets Manager secret is encrypted with the default AWS Key Management Service (AWS KMS) key.A DevOps engineer needs to update the infrastructure to ensure that only the Lambda function‚Äôs execution role can access the values in Secrets Manager. The solution must apply the principle of least privilege.Which combination of steps will meet these requirements? (Choose two.)",
        "options": [
            "Create a KMS customer managed key that trusts Secrets Manager and allows the account's root principal to decrypt. Update Secrets Manager to use the new customer managed key",
            "Ensure that the Lambda function‚Äôs execution role has the KMS permissions scoped on the resource level. Configure the permissions so that the KMS key can encrypt the Secrets Manager secret",
            "Create a KMS customer managed key that trusts Secrets Manager and allows the Lambda function's execution role to decrypt. Update Secrets Manager to use the new customer managed key",
            "Update the default KMS key for Secrets Manager to allow only the Lambda function‚Äôs execution role to decrypt",
            "Remove all KMS permissions from the Lambda function‚Äôs execution role"
        ],
        "correct": [
            1,
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 74,
        "text": "AnyCompany is using AWS Organizations to create and manage multiple AWS accounts. AnyCompany recently acquired a smaller company, Example Corp. During the acquisition process, Example Corp's single AWS account joined AnyCompany's management account through an Organizations invitation. AnyCompany moved the new member account under an OU that is dedicated to Example Corp.AnyCompany's DevOps engineer has an IAM user that assumes a role that is named OrganizationAccountAccessRole to access member accounts. This role is configured with a full access policy. When the DevOps engineer tries to use the AWS Management Console to assume the role in Example Corp's new member account, the DevOps engineer receives the following error message: \"Invalid information in one or more fields. Check your information or contact your administrator.\"Which solution will give the DevOps engineer access to the new member account?",
        "options": [
            "In the new member account, edit the trust policy for the OrganizationAccountAccessRole IAM role. Grant the management account permission to assume the role.",
            "In the management account, grant the DevOps engineer's IAM user permission to assume the OrganizationAccountAccessRole IAM role in the new member account.",
            "In the management account, create a new SCP. In the SCP, grant the DevOps engineer's IAM user full access to all resources in the new member account. Attach the SCP to the OU that contains the new member account.",
            "In the new member account, create a new IAM role that is named OrganizationAccountAccessRole. Attach the AdministratorAccess AWS managed policy to the role. In the role's trust policy, grant the management account permission to assume the role."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 75,
        "text": "A company plans to use Amazon CloudWatch to monitor its Amazon EC2 instances. The company needs to stop EC2 instances when the average of the NetworkPacketsIn metric is less than 5 for at least 3 hours in a 12-hour time window. The company must evaluate the metric every hour. The EC2 instances must continue to run if there is missing data for the NetworkPacketsIn metric during the evaluation period.A DevOps engineer creates a CloudWatch alarm for the NetworkPacketsIn metric. The DevOps engineer configures a threshold value of 5 and an evaluation period of 1 hour.Which set of additional actions should the DevOps engineer take to meet these requirements?",
        "options": [
            "Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.",
            "Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.",
            "Configure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.",
            "Configure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 76,
        "text": "A DevOps engineer wants to find a solution to migrate an application from on premises to AWS. The application is running on Linux and needs to run on specific versions of Apache Tomcat, HAProxy, and Varnish Cache to function properly. The application's operating system-level parameters require tuning. The solution must include a way to automate the deployment of new application versions. The infrastructure should be scalable and faulty servers should be replaced automatically.Which solution should the DevOps engineer use?",
        "options": [
            "Upload the application code to an AWS CodeCommit repository with an appspec.yml file to configure and install the necessary software. Create an AWS CodeDeploy deployment group associated with an Amazon EC2 Auto Scaling group. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and CodeDeploy as a deployment provider.",
            "Upload the application code to an AWS CodeCommit repository with a set of .ebextensions files to configure and install the software. Create an AWS Elastic Beanstalk worker tier environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.",
            "Upload the application code to an AWS CodeCommit repository with a saved configuration file to configure and install the software. Create an AWS Elastic Beanstalk web server tier and a load balanced-type environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.",
            "Upload the application as a Docker image that contains all the necessary software to Amazon ECR. Create an Amazon ECS cluster using an AWS Fargate launch type and an Auto Scaling group. Create an AWS CodePipeline pipeline that uses Amazon ECR as a source and Amazon ECS as a deployment provider."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 77,
        "text": "A DevOps engineer has created an AWS CloudFormation template that deploys an application on Amazon EC2 instances. The EC2 instances run Amazon Linux. The application is deployed to the EC2 instances by using shell scripts that contain user data. The EC2 instances have an IAM instance profile that has an IAM role with the AmazonSSMManagedinstanceCore managed policy attached.The DevOps engineer has modified the user data in the CloudFormation template to install a new version of the application. The engineer has also applied the stack update. However, the application was not updated on the running EC2 instances. The engineer needs to ensure that the changes to the application are installed on the running EC2 instances.Which combination of steps will meet these requirements? (Choose two.)",
        "options": [
            "Refactor the user data command to use an AWS Systems Manager document (SSM document). Use Systems Manager State Manager to create an association between the SSM document and the EC2 instances.",
            "Refactor the user data commands to use the cfn-init helper script. Update the user data to install and configure the cfn-hup and cfn-init helper scripts to monitor and apply the metadata changes.",
            "Refactor the user data commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.",
            "Configure the user data content to use the Multipurpose Internet Mail Extensions (MIME) multipart format. Set the scripts-user parameter to always in the text/cloud-config section.",
            "Configure an EC2 launch template for the EC2 instances. Create a new EC2 Auto Scaling group. Associate the Auto Scaling group with the EC2 launch template. Use the AutoScalingScheduledAction update policy for the Auto Scaling group."
        ],
        "correct": [
            0,
            1
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 78,
        "text": "A large enterprise is deploying a web application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon RDS for Oracle DB instance and Amazon DynamoDB. There are separate environments for development, testing, and production.What is the MOST secure and flexible way to obtain password credentials during deployment?",
        "options": [
            "Retrieve an access key from an AWS Systems Manager plaintext parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.",
            "Launch the EC2 instances with an EC2 IAM role to access AWS services. Store the database passwords in an encrypted config file with the application artifacts.",
            "Launch the EC2 instances with an EC2 IAM role to access AWS services. Retrieve the database credentials from AWS Secrets Manager.",
            "Retrieve an access key from an AWS Systems Manager SecureString parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 79,
        "text": "A company has an AWS Control Tower landing zone that manages its organization in AWS Organizations. The company created an OU structure that is based on the company's requirements. The company's DevOps team has established the core accounts for the solution and an account for all centralized AWS CloudFormation and AWS Service Catalog solutions.The company wants to offer a series of customizations that an account can request through AWS Control Tower.Which combination of steps will meet these requirements? (Choose three.)",
        "options": [
            "Create a CloudFormation template that contains the resources for each customization.",
            "Create a Service Catalog product for each CloudFormation template.",
            "Create an IAM role that is named AWSControlTowerBlueprintAccess. Configure the role with a trust policy that allows the AWSControlTowerAdmin role in the management account to assume the role. Attach the AWSServiceCatalogAdminFullAccess IAM policy to the AWSControlTowerBlueprintAccess role.",
            "Deploy the Customizations for AWS Control Tower (CfCT) CloudFormation stack.",
            "Create a CloudFormation stack set for each CloudFormation template. Enable automatic deployment for each stack set. Create a CloudFormation stack instance that targets specific OUs.",
            "Enable trusted access for CloudFormation with Organizations by using service-managed permissions."
        ],
        "correct": [
            0,
            1,
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 80,
        "text": "A company has multiple development groups working in a single shared AWS account. The senior manager of the groups wants to be alerted via a third-party API call when the creation of resources approaches the service limits for the account.Which solution will accomplish this with the LEAST amount of development effort?",
        "options": [
            "Add an AWS Config custom rule that runs periodically, checks the AWS service limit status, and streams notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Deploy an AWS Lambda function that notifies the senior manager, and subscribe the Lambda function to the SNS topic.",
            "Deploy an AWS Lambda function that refreshes AWS Health Dashboard checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Health Dashboard events and a target Lambda function. In the target Lambda function, notify the senior manager.",
            "Deploy an AWS Lambda function that refreshes AWS Trusted Advisor checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Trusted Advisor events and a target Lambda function. In the target Lambda function, notify the senior manager.",
            "Create an Amazon EventBridge rule that runs periodically and targets an AWS Lambda function. Within the Lambda function, evaluate the current state of the AWS environment and compare deployed resource values to resource limits on the account. Notify the senior manager if the account is approaching a service limit."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 81,
        "text": "A company uses AWS WAF to protect its cloud infrastructure. A DevOps engineer needs to give an operations team the ability to analyze log messages from AWS WAF. The operations team needs to be able to create alarms for specific patterns in the log output.Which solution will meet these requirements with the LEAST operational overhead?",
        "options": [
            "Create an Amazon OpenSearch Service cluster and appropriate indexes. Configure an Amazon Kinesis Data Firehose delivery stream to stream log data to the indexes. Use OpenSearch Dashboards to create filters and widgets.",
            "Create an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Instruct the operations team to create AWS Lambda functions that detect each desired log message pattern. Configure the Lambda functions to publish to an Amazon Simple Notification Service (Amazon SNS) topic.",
            "Create an Amazon CloudWatch Logs log group. Configure the appropriate AWS WAF web ACL to send log messages to the log group. Instruct the operations team to create CloudWatch metric filters.",
            "Create an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Use Amazon Athena to create an external table definition that fits the log message pattern. Instruct the operations team to write SQL queries and to create Amazon CloudWatch metric filters for the Athena queries."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 82,
        "text": "A highly regulated company has a policy that DevOps engineers should not log in to their Amazon EC2 instances except in emergencies. If a DevOps engineer does log in, the security team must be notified within 15 minutes of the occurrence.Which solution will meet these requirements?",
        "options": [
            "Install the Amazon CloudWatch agent on each EC2 instance. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user logins. If a login is found, send a notification to the security team using Amazon SNS.",
            "Set up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event to invoke an AWS Lambda function, which invokes an Amazon Athena query to run. The Athena query checks for logins and sends the output to the security team using Amazon SNS.",
            "Set up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Attach AWS Lambda to Kinesis to parse and determine if a log contains a user login. If it does, send a notification to the security team using Amazon SNS.",
            "Install the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon EventBridge notifications. Invoke an AWS Lambda function to check if a message is about user logins. If it is, send a notification to the security team using Amazon SNS."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 83,
        "text": "A company manages AWS accounts for application teams in AWS Control Tower. Individual application teams are responsible for securing their respective AWS accounts.A DevOps engineer needs to enable Amazon GuardDuty for all AWS accounts in which the application teams have not already enabled GuardDuty. The DevOps engineer is using AWS CloudFormation StackSets from the AWS Control Tower management account.How should the DevOps engineer configure the CloudFormation template to prevent failure during the StackSets deployment?",
        "options": [
            "Use the CloudFormation Fn::GetAtt intrinsic function to check whether GuardDuty is already enabled. If GuardDuty is not already enabled, use the Resources section of the CloudFormation template to enable GuardDuty.",
            "Manually discover the list of AWS account IDs where GuardDuty is not enabled. Use the CloudFormation Fn::ImportValue intrinsic function to import the list of account IDs into the CloudFormation template to skip deployment for the listed AWS accounts.",
            "Use the Conditions section of the CloudFormation template to enable GuardDuty in accounts where GuardDuty is not already enabled.",
            "Create a CloudFormation custom resource that invokes an AWS Lambda function. Configure the Lambda function to conditionally enable GuardDuty if GuardDuty is not already enabled in the accounts."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 84,
        "text": "A company uses an organization in AWS Organizations to manage its AWS accounts. The company recently acquired another company that has standalone AWS accounts. The acquiring company's DevOps team needs to consolidate the administration of the AWS accounts for both companies and retain full administrative control of the accounts. The DevOps team also needs to collect and group findings across all the accounts to implement and maintain a security posture.Which combination of steps should the DevOps team take to meet these requirements? (Choose two.)",
        "options": [
            "Use Amazon Inspector to collect and group findings across all accounts. Designate an account in the organization as the delegated administrator account for Amazon Inspector.",
            "Use AWS Security Hub to collect and group findings across all accounts. Use Security Hub to automatically detect new accounts as the accounts are added to the organization.",
            "Invite the acquired company's AWS accounts to join the organization. Create an SCP that has full administrative privileges. Attach the SCP to the management account.",
            "Use AWS Firewall Manager to collect and group findings across all accounts. Enable all features for the organization. Designate an account in the organization as the delegated administrator account for Firewall Manager.",
            "Invite the acquired company's AWS accounts to join the organization. Create the OrganizationAccountAccessRole IAM role in the invited accounts. Grant permission to the management account to assume the role."
        ],
        "correct": [
            1,
            4
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 85,
        "text": "A company runs a workload on Amazon EC2 instances. The company needs a control that requires the use of Instance Metadata Service Version 2 (IMDSv2) on all EC2 instances in the AWS account. If an EC2 instance does not prevent the use of Instance Metadata Service Version 1 (IMDSv1), the EC2 instance must be terminated.Which solution will meet these requirements?",
        "options": [
            "Set up Amazon Inspector in the account. Configure Amazon Inspector to activate deep inspection for EC2 instances. Create an Amazon EventBridge rule for an Inspector2 finding. Set an AWS Lambda function as the target to terminate the instance.",
            "Create a permissions boundary that prevents the ec2:RunInstance action if the ec2:MetadataHttpTokens condition key is not set to a value of required. Attach the permissions boundary to the IAM role that was used to launch the instance.",
            "Set up AWS Config in the account. Use a managed rule to check EC2 instances. Configure the rule to remediate the findings by using AWS Systems Manager Automation to terminate the instance.",
            "Create an Amazon EventBridge rule for the EC2 instance launch successful event. Send the event to an AWS Lambda function to inspect the EC2 metadata and to terminate the instance."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 86,
        "text": "A company is launching an application that stores raw data in an Amazon S3 bucket. Three applications need to access the data to generate reports. The data must be redacted differently for each application before the applications can access the data.Which solution will meet these requirements?",
        "options": [
            "For each application, create an S3 access point that uses the raw data's S3 bucket as the destination. Create an AWS Lambda function that is invoked by object creation events in the raw data's S3 bucket. Program the Lambda function to redact data for each application. Store the data in each application's S3 access point. Configure each application to consume data from its own S3 access point.",
            "Create an S3 access point that uses the raw data‚Äôs S3 bucket as the destination. For each application, create an S3 Object Lambda access point that uses the S3 access point. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved. Configure each application to consume data from its own S3 Object Lambda access point",
            "Create an S3 bucket for each application. Configure S3 Same-Region Replication (SRR) from the raw data's S3 bucket to each application's S3 bucket. Configure each application to consume data from its own S3 bucket.",
            "Create an Amazon Kinesis data stream. Create an AWS Lambda function that is invoked by object creation events in the raw data‚Äôs S3 bucket. Program the Lambda function to redact data for each application. Publish the data on the Kinesis data stream. Configure each application to consume data from the Kinesis data stream."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 87,
        "text": "A company manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. The application uses an Amazon RDS for MySQL DB instance to store the data. The company has configured Amazon Route 53 with an alias record that points to the ALB.A new company guideline requires a geographically isolated disaster recovery (DR) site with an RTO of 4 hours and an RPO of 15 minutes.Which DR strategy will meet these requirements with the LEAST change to the application stack?",
        "options": [
            "Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy. In the event of an outage, promote the read replica to primary.",
            "Launch a replica environment of everything except Amazon RDS in a different Availability Zone. Create an RDS read replica in the new Availability Zone, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy.",
            "Launch a replica environment of everything except Amazon RDS in a different AWS Region. In the event of an outage, copy and restore the latest RDS snapshot from the primary Region to the DR Region. Adjust the Route 53 record set to point to the ALB in the DR Region.",
            "Launch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a latency routing policy."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 88,
        "text": "A company recently deployed its web application on AWS. The company is preparing for a large-scale sales event and must ensure that the web application can scale to meet the demand.The application's frontend infrastructure includes an Amazon CloudFront distribution that has an Amazon S3 bucket as an origin. The backend infrastructure includes an Amazon API Gateway API, several AWS Lambda functions, and an Amazon Aurora DB cluster.The company's DevOps engineer conducts a load test and identifies that the Lambda functions can fulfil the peak number of requests. However, the DevOps engineer notices request latency during the initial burst of requests. Most of the requests to the Lambda functions produce queries to the database. A large portion of the invocation time is used to establish database connections.Which combination of steps will provide the application with the required scalability? (Choose three.)",
        "options": [
            "Configure a higher reserved concurrency for the Lambda functions.",
            "Convert the DB cluster to an Aurora global database. Add additional Aurora Replicas in AWS Regions based on the locations of the company's customers.",
            "Configure a higher provisioned concurrency for the Lambda functions.",
            "Use Amazon RDS Proxy to create a proxy for the Aurora database. Update the Lambda functions to use the proxy endpoints for database connections.",
            "Refactor the Lambda functions. Move the code blocks that initialize database connections into the function handlers."
        ],
        "correct": [
            1,
            2
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 89,
        "text": "A company needs to ensure that flow logs remain configured for all existing and new VPCs in its AWS account. The company uses an AWS CloudFormation stack to manage its VPCs. The company needs a solution that will work for any VPCs that any IAM user creates.Which solution will meet these requirements?",
        "options": [
            "Turn on AWS Config. Create an AWS Config rule to check whether VPC flow logs are turned on. Configure automatic remediation to turn on VPC flow logs.",
            "Create an IAM policy to deny the use of API calls for VPC flow logs. Attach the IAM policy to all IAM users.",
            "Create an organization in AWS Organizations. Add the company's AWS account to the organization. Create an SCP to prevent users from modifying VPC flow logs.",
            "Add the AWS::EC2::FlowLog resource to the CloudFormation stack that creates the VPCs."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 90,
        "text": "A company has an application that includes AWS Lambda functions. The Lambda functions run Python code that is stored in an AWS CodeCommit repository. The company has recently experienced failures in the production environment because of an error in the Python code. An engineer has written unit tests for the Lambda functions to help avoid releasing any future defects into the production environment.The company's DevOps team needs to implement a solution to integrate the unit tests into an existing AWS CodePipeline pipeline. The solution must produce reports about the unit tests for the company to view.Which solution will meet these requirements?",
        "options": [
            "Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a new Amazon S3 bucket. Create a buildspec.yml file in the CodeCommit repository. In the buildspec yml file, define the actions to run the unit tests with an output of HTML in the phases section. In the reports section, upload the test reports to the S3 bucket.",
            "Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a CodeBuild report group. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run the unit tests with an output of JUNITXML in the build phase section. Configure the test reports to be uploaded to the new CodeBuild report group.",
            "Create a new AWS CodeArtifact repository. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create an appspec.yml file in the original CodeCommit repository. In the appspec.yml file, define the actions to run the unit tests with an output of CUCUMBERJSON in the build phase section. Configure the tests reports to be sent to the new CodeArtifact repository.",
            "Associate the CodeCommit repository with Amazon CodeGuru Reviewer. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a buildspec.yml file in the CodeCommit repository. In the buildspec yml file, define the actions to run a CodeGuru review."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 91,
        "text": "A media company has several thousand Amazon EC2 instances in an AWS account. The company is using Slack and a shared email inbox for team communications and important updates. A DevOps engineer needs to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox. The solution must include the instances‚Äô Name and Owner tags.Which solution will meet these requirements?",
        "options": [
            "Configure AWS Support integration with AWS CloudTrail. Create a CloudTrail lookup event to invoke an AWS Lambda function to pass EC2 maintenance notifications to Amazon Simple Notification Service (Amazon SNS). Configure Amazon SNS to target the Slack channel and the shared inbox.",
            "Create an AWS Lambda function that sends EC2 maintenance notifications to the Slack channel and the shared inbox. Monitor EC2 health events by using Amazon CloudWatch metrics. Configure a CloudWatch alarm that invokes the Lambda function when a maintenance notification is received.",
            "Use Amazon EventBridge to monitor for AWS Health events. Configure the maintenance events to target an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to send notifications to the Slack channel and the shared inbox.",
            "Integrate AWS Trusted Advisor with AWS Config. Configure a custom AWS Config rule to invoke an AWS Lambda function to publish notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe a Slack channel endpoint and the shared inbox to the topic."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 92,
        "text": "A company updated the AWS CloudFormation template for a critical business application. The stack update process failed due to an error in the updated template, and AWS CloudFormation automatically began the stack rollback process. Later, a DevOps engineer discovered that the application was still unavailable and that the stack was in the UPDATE_ROLLBACK_FAILED state.Which combination of actions should the DevOps engineer perform so that the stack rollback can complete successfully? (Choose two.)",
        "options": [
            "Attach the AWSCloudFormationFullAccess IAM policy to the AWS CloudFormation role.",
            "Manually adjust the resources to match the expectations of the stack.",
            "Automatically recover the stack resources by using AWS CloudFormation drift detection.",
            "Update the existing AWS CloudFormation stack by using the original template.",
            "Issue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI."
        ],
        "correct": [
            1,
            4
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 93,
        "text": "A DevOps engineer is planning to deploy a Ruby-based application to production. The application needs to interact with an Amazon RDS for MySQL database and should have automatic scaling and high availability. The stored data in the database is critical and should persist regardless of the state of the application stack.The DevOps engineer needs to set up an automated deployment strategy for the application with automatic rollbacks. The solution also must alert the application team when a deployment fails.Which combination of steps will meet these requirements? (Choose three.)",
        "options": [
            "Configure a notification email address that alerts the application team in the AWS Elastic Beanstalk configuration.",
            "Use the immutable deployment method to deploy new application versions.",
            "Use the rolling deployment method to deploy new application versions.",
            "Deploy the application on AWS Elastic Beanstalk. Deploy a separate Amazon RDS for MySQL DB instance outside of Elastic Beanstalk.",
            "Deploy the application on AWS Elastic Beanstalk. Deploy an Amazon RDS for MySQL DB instance as part of the Elastic Beanstalk configuration.",
            "Configure an Amazon EventBridge rule to monitor AWS Health events. Use an Amazon Simple Notification Service (Amazon SNS) topic as a target to alert the application team."
        ],
        "correct": [
            0,
            1,
            3
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 94,
        "text": "A company uses Amazon RDS for all databases in its AWS accounts. The company uses AWS Control Tower to build a landing zone that has an audit and logging account. All databases must be encrypted at rest for compliance reasons. The company's security engineer needs to receive notification about any noncompliant databases that are in the company‚Äôs accounts.Which solution will meet these requirements with the MOST operational efficiency?",
        "options": [
            "Use AWS Control Tower to activate the optional detective control (guardrail) to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the company's audit account. Create an Amazon EventBridge rule to filter noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer's email address to the SNS topic.",
            "Use AWS CloudFormation StackSets to deploy AWS Lambda functions to every account. Write the Lambda function code to determine whether the RDS storage is encrypted in the account the function is deployed to. Send the findings as an Amazon CloudWatch metric to the management account. Create an Amazon Simple Notification Service (Amazon SNS) topic. Create a CloudWatch alarm that notifies the SNS topic when metric thresholds are met. Subscribe the security engineer's email address to the SNS topic.",
            "Create a custom AWS Config rule in every account to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the audit account. Create an Amazon EventBidge rule to filter noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer's email address to the SNS topic.",
            "Launch an Amazon C2 instance. Run an hourly cron job by using the AWS CLI to determine whether the RDS storage is encrypted in each AWS account. Store the results in an RDS database. Notify the security engineer by sending email messages from the EC2 instance when noncompliance is detected"
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 95,
        "text": "A company has an AWS CodeDeploy application. The application has a deployment group that uses a single tag group to identify instances for the deployment of Application. The single tag group configuration identifies instances that have Environment=Production and Name=ApplicationA tags for the deployment of ApplicationA.The company launches an additional Amazon EC2 instance with Department=Marketing, Environment=Production, and Name=ApplicationB tags. On the next CodeDeploy deployment of Application, the additional instance has ApplicationA installed on it. A DevOps engineer needs to configure the existing deployment group to prevent ApplicationA from being installed on the additional instance.Which solution will meet these requirements?",
        "options": [
            "Add another single tag group that includes only the Department=Marketing tag. Keep the Environment=Production and Name=ApplicationA tags with the current single tag group.",
            "Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Department=Marketing tag.",
            "Change the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Name=ApplicationA tag.",
            "Change the current single tag group to include the Department=Marketing, Environment=production, and Name=ApplicationA tags."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 96,
        "text": "A company uses AWS CodeCommit for source code control. Developers apply their changes to various feature branches and create pull requests to move those changes to the main branch when the changes are ready for production.The developers should not be able to push changes directly to the main branch. The company applied the AWSCodeCommitPowerUser managed policy to the developers‚Äô IAM role, and now these developers can push changes to the main branch directly on every repository in the AWS account.What should the company do to restrict the developers‚Äô ability to push changes to the main branch directly?",
        "options": [
            "Remove the IAM policy, and add an AWSCodeCommitReadOnly managed policy. Add an Allow rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.",
            "Create an additional policy to include a Deny rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the main branch.",
            "Modify the IAM policy. Include a Deny rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.",
            "Create an additional policy to include an Allow rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the feature branches."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 97,
        "text": "A DevOps engineer notices that all Amazon EC2 instances running behind an Application Load Balancer in an Auto Scaling group are failing to respond to user requests. The EC2 instances are also failing target group HTTP health checks.Upon inspection, the engineer notices the application process was not running in any EC2 instances. There are a significant number of out of memory messages in the system logs. The engineer needs to improve the resilience of the application to cope with a potential application memory leak. Monitoring and notifications should be enabled to alert when there is an issue.Which combination of actions will meet these requirements? (Choose two.)",
        "options": [
            "Enable the available memory consumption metric within the Amazon CloudWatch dashboard for the entire Auto Scaling group. Create an alarm when the memory utilization is high. Associate an Amazon SNS topic to the alarm to receive notifications when the alarm goes off.",
            "Change the Auto Scaling configuration to replace the instances when they fail the load balancer's health checks.",
            "Change the target group health check HealthCheckIntervalSeconds parameter to reduce the interval between health checks.",
            "Change the target group health checks from HTTP to TCP to check if the port where the application is listening is reachable.",
            "Use the Amazon CloudWatch agent to collect the memory utilization of the EC2 instances in the Auto Scaling group. Create an alarm when the memory utilization is high and associate an Amazon SNS topic to receive a notification."
        ],
        "correct": [
            1,
            4
        ],
        "type": "multiple",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 98,
        "text": "A company has microservices running in AWS Lambda that read data from Amazon DynamoDB. The Lambda code is manually deployed by developers after successful testing. The company now needs the tests and deployments be automated and run in the cloud. Additionally, traffic to the new versions of each microservice should be incrementally shifted over time after deployment.What solution meets all the requirements, ensuring the MOST developer velocity?",
        "options": [
            "Create an AWS CodeBuild configuration that triggers when the test code is pushed. Use AWS CloudFormation to trigger an AWS CodePipeline configuration that deploys the new Lambda versions and specifies the traffic shift percentage and interval.",
            "Create an AWS CodePipeline configuration and set up the source code step to trigger when code is pushed. Set up the build step to use AWS CodeBuild to run the tests. Set up an AWS CodeDeploy configuration to deploy, then select the CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option.",
            "Create an AWS CodePipeline configuration and set up a post-commit hook to trigger the pipeline after tests have passed. Use AWS CodeDeploy and create a Canary deployment configuration that specifies the percentage of traffic and interval.",
            "Use the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket after tests have passed Set up an S3 event trigger that runs a Lambda function that deploys the new version. Use an interval in the Lambda function to deploy the code over time at the required percentage."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 99,
        "text": "A company uses AWS Directory Service for Microsoft Active Directory as its identity provider (IdP). The company requires all infrastructure to be defined and deployed by AWS CloudFormation.A DevOps engineer needs to create a fleet of Windows-based Amazon EC2 instances to host an application. The DevOps engineer has created a CloudFormation template that contains an EC2 launch template, IAM role, EC2 security group, and EC2 Auto Scaling group. The DevOps engineer must implement a solution that joins all EC2 instances to the domain of the AWS Managed Microsoft AD directory.Which solution will meet these requirements with the MOST operational efficiency?",
        "options": [
            "Store the existing AWS Managed Microsoft AD domain administrator credentials in AWS Secrets Manager. In the CloudFormation template, update the EC2 launch template to include user data. Configure the user data to pull the administrator credentials from Secrets Manager and to join the AWS Managed Microsoft AD domain. Attach the AmazonSSMManagedInstanceCore and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use.",
            "In the CloudFormation template, update the launch template to include specific tags that propagate on launch. Create an AWS::SSM::Association resource to associate the AWS-JoinDirectoryServiceDomain Automation runbook with the EC2 instances that have the specified tags. Define the required parameters to join the AWS Managed Microsoft AD directory. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use.",
            "In the CloudFormation template, create an AWS::SSM::Document resource that joins the EC2 instance to the AWS Managed Microsoft AD domain by using the parameters for the existing directory. Update the launch template to include the SSMAssociation property to use the new SSM document. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use.",
            "Store the existing AWS Managed Microsoft AD domain connection details in AWS Secrets Manager. In the CloudFormation template, create an AWS::SSM::Association resource to associate the AWS-CreateManagedWindowsInstanceWithApproval Automation runbook with the EC2 Auto Scaling group. Pass the ARNs for the parameters from Secrets Manager to join the domain. Attach the AmazonSSMDirectoryServiceAccess and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use."
        ],
        "correct": [
            1
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 100,
        "text": "A company manages a multi-tenant environment in its VPC and has configured Amazon GuardDuty for the corresponding AWS account. The company sends all GuardDuty findings to AWS Security Hub.Traffic from suspicious sources is generating a large number of findings. A DevOps engineer needs to implement a solution to automatically deny traffic across the entire VPC when GuardDuty discovers a new suspicious source.Which solution will meet these requirements?",
        "options": [
            "Create an AWS Lambda function that will create a GuardDuty suppression rule. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.",
            "Create a GuardDuty threat list. Configure GuardDuty to reference the list. Create an AWS Lambda function that will update the threat list. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.",
            "Configure an AWS WAF web ACL that includes a custom rule group. Create an AWS Lambda function that will create a block rule in the custom rule group. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.",
            "Configure a firewall in AWS Network Firewall. Create an AWS Lambda function that will create a Drop action rule in the firewall policy. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 101,
        "text": "A company runs an application for multiple environments in a single AWS account. An AWS CodePipeline pipeline uses a development Amazon Elastic Container Service (Amazon ECS) cluster to test an image for the application from an Amazon Elastic Container Registry (Amazon ECR) repository. The pipeline promotes the image to a production ECS cluster.The company needs to move the production cluster into a separate AWS account in the same AWS Region. The production cluster must be able to download the images over a private connection.Which solution will meet these requirements?",
        "options": [
            "Set a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.",
            "Configure ECR private image replication in the main AWS account. Activate cross-account replication. Define the destination account ID of the separate AWS account.",
            "Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. In the separate AWS account, create an ECR repository. Set the repository policy to allow the production ECS tasks to pull images from the main AWS account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.",
            "Use Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. Set a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository."
        ],
        "correct": [
            3
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 102,
        "text": "A company has a legacy application. A DevOps engineer needs to automate the process of building the deployable artifact for the legacy application. The solution must store the deployable artifact in an existing Amazon S3 bucket for future deployments to reference.Which solution will meet these requirements in the MOST operationally efficient way?",
        "options": [
            "Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Configure a new AWS CodeBuild project to use the custom Docker image to build the deployable artifact and to save the artifact to the S3 bucket.",
            "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with an AWS Fargate profile that runs in multiple Availability Zones. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Use the custom Docker image inside the EKS cluster to build the deployable artifact and to save the artifact to the S3 bucket.",
            "Launch a new Amazon EC2 instance. Install all the dependencies for the legacy application on the EC2 instance. Use the EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.",
            "Create a custom EC2 Image Builder image. Install all the dependencies for the legacy application on the image. Launch a new Amazon EC2 instance from the image. Use the new EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket."
        ],
        "correct": [
            0
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    },
    {
        "id": 103,
        "text": "A company is using AWS Organizations to create separate AWS accounts for each of its departments. The company needs to automate the following tasks:‚Ä¢ Update the Linux AMIs with new patches periodically and generate a golden image‚Ä¢ Install a new version of Chef agents in the golden image, if available‚Ä¢ Provide the newly generated AMIs to the department's accountsWhich solution meets these requirements with the LEAST management overhead?",
        "options": [
            "Write a script to launch an Amazon EC2 instance from the previous golden image. Apply the patch updates. Install the new version of the Chef agent, generate a new golden image, and then modify the AMI permissions to share only the new image with the department's accounts.",
            "Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Create a parameter in AWS Systems Manager Parameter Store to store the new AMI ID that can be referenced by the department's accounts.",
            "Use Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Use AWS Resource Access Manager to share EC2 Image Builder images with the department's accounts.",
            "Use an AWS Systems Manager Automation runbook to update the Linux AMI by using the previous image. Provide the URL for the script that will update the Chef agent. Use AWS Organizations to replace the previous golden image in the department's accounts."
        ],
        "correct": [
            2
        ],
        "type": "single",
        "explanation": "ƒê√°p √°n ƒë∆∞·ª£c ch·ªçn nhi·ªÅu nh·∫•t trong c·ªông ƒë·ªìng. Xem th√™m gi·∫£i th√≠ch trong file g·ªëc."
    }
]
        let currentQuestionIndex = 0;
        let userAnswers = [];
        let startTime = Date.now();
        let timer;

        function initializeApp() {
            document.getElementById('totalQuestions').textContent = questions.length;
            loadQuestion();
            startTimer();
            updateStats();
        }

        function loadQuestion() {
            const question = questions[currentQuestionIndex];
            document.getElementById('questionText').innerHTML = `
                <strong>C√¢u ${question.id}:</strong><br><br>
                ${question.text}
            `;
            
            // Set question type indicator
            const typeElement = document.getElementById('questionType');
            if (question.type === 'multiple') {
                typeElement.textContent = `Multiple Choice (Choose ${question.correct.length})`;
                typeElement.style.background = '#fff3cd';
                typeElement.style.color = '#856404';
            } else {
                typeElement.textContent = 'Single Choice';
                typeElement.style.background = '#e3f2fd';
                typeElement.style.color = '#1976d2';
            }
            
            const optionsContainer = document.getElementById('optionsContainer');
            optionsContainer.innerHTML = '';
            
            question.options.forEach((option, index) => {
                const optionDiv = document.createElement('div');
                optionDiv.className = 'option';
                optionDiv.innerHTML = `
                    <strong>${String.fromCharCode(65 + index)}.</strong> ${option}
                    ${question.type === 'multiple' ? '<input type="checkbox" class="option-checkbox">' : ''}
                `;
                
                if (question.type === 'multiple') {
                    optionDiv.onclick = () => toggleMultipleOption(index);
                } else {
                    optionDiv.onclick = () => selectSingleOption(index);
                }
                
                optionDiv.dataset.index = index;
                optionsContainer.appendChild(optionDiv);
            });

            document.getElementById('currentQuestion').textContent = currentQuestionIndex + 1;
            document.getElementById('submitBtn').disabled = true;
            document.getElementById('nextBtn').disabled = true;
            updateProgressBar();
        }

        function selectSingleOption(index) {
            document.querySelectorAll('.option').forEach(opt => opt.classList.remove('selected'));
            document.querySelector(`[data-index="${index}"]`).classList.add('selected');
            document.getElementById('submitBtn').disabled = false;
        }

        function toggleMultipleOption(index) {
            const option = document.querySelector(`[data-index="${index}"]`);
            const checkbox = option.querySelector('.option-checkbox');
            
            if (option.classList.contains('selected')) {
                option.classList.remove('selected');
                checkbox.checked = false;
            } else {
                option.classList.add('selected');
                checkbox.checked = true;
            }
            
            // Enable submit if at least one option is selected
            const selectedCount = document.querySelectorAll('.option.selected').length;
            document.getElementById('submitBtn').disabled = selectedCount === 0;
        }

        function getSelectedAnswers() {
            const question = questions[currentQuestionIndex];
            const selected = [];
            
            document.querySelectorAll('.option.selected').forEach(opt => {
                selected.push(parseInt(opt.dataset.index));
            });
            
            return selected.sort((a, b) => a - b); // Sort for comparison
        }

        function submitAnswer() {
            const selectedAnswers = getSelectedAnswers();
            if (selectedAnswers.length === 0) return;

            const question = questions[currentQuestionIndex];
            const correctAnswers = [...question.correct].sort((a, b) => a - b);
            
            // Calculate correctness
            let isCorrect = false;
            let isPartial = false;
            
            if (question.type === 'single') {
                isCorrect = selectedAnswers.length === 1 && selectedAnswers[0] === correctAnswers[0];
            } else {
                // Multiple choice scoring
                const selectedSet = new Set(selectedAnswers);
                const correctSet = new Set(correctAnswers);
                
                if (selectedAnswers.length === correctAnswers.length && 
                    selectedAnswers.every(ans => correctSet.has(ans))) {
                    isCorrect = true;
                } else {
                    // Partial credit if some answers are correct
                    const correctSelected = selectedAnswers.filter(ans => correctSet.has(ans));
                    const incorrectSelected = selectedAnswers.filter(ans => !correctSet.has(ans));
                    
                    if (correctSelected.length > 0 && incorrectSelected.length === 0) {
                        isPartial = true; // Selected some correct, no incorrect
                    }
                }
            }

            userAnswers[currentQuestionIndex] = {
                questionId: question.id,
                selected: selectedAnswers,
                correct: correctAnswers,
                isCorrect: isCorrect,
                isPartial: isPartial,
                timeSpent: Date.now() - startTime,
                type: question.type
            };

            // Show results
            document.querySelectorAll('.option').forEach((opt, index) => {
                const isSelectedCorrect = correctAnswers.includes(index);
                const isSelectedByUser = selectedAnswers.includes(index);
                
                if (isSelectedCorrect) {
                    opt.classList.add('correct');
                } else if (isSelectedByUser) {
                    opt.classList.add('incorrect');
                }
                
                opt.onclick = null; // Disable clicking
            });

            // Show explanation
            const explanationDiv = document.createElement('div');
            let resultClass = 'incorrect';
            let resultText = '‚ùå Sai r·ªìi!';
            
            if (isCorrect) {
                resultClass = 'correct';
                resultText = '‚úÖ Ch√≠nh x√°c!';
            } else if (isPartial) {
                resultClass = 'partial';
                resultText = '‚ö†Ô∏è ƒê√∫ng m·ªôt ph·∫ßn!';
            }
            
            explanationDiv.className = `explanation ${resultClass}`;
            explanationDiv.innerHTML = `
                <strong>${resultText}</strong><br>
                <strong>ƒê√°p √°n ƒë√∫ng:</strong> ${correctAnswers.map(i => String.fromCharCode(65 + i)).join(', ')}<br>
                <strong>B·∫°n ch·ªçn:</strong> ${selectedAnswers.map(i => String.fromCharCode(65 + i)).join(', ')}<br>
                <em>Gi·∫£i th√≠ch:</em> ${question.explanation}
            `;
            document.getElementById('optionsContainer').appendChild(explanationDiv);

            document.getElementById('submitBtn').style.display = 'none';
            document.getElementById('nextBtn').disabled = false;
            updateStats();
        }

        function nextQuestion() {
            if (currentQuestionIndex < questions.length - 1) {
                currentQuestionIndex++;
                loadQuestion();
                document.getElementById('submitBtn').style.display = 'inline-block';
            } else {
                showReview();
            }
        }

        function previousQuestion() {
            if (currentQuestionIndex > 0) {
                currentQuestionIndex--;
                loadQuestion();
                document.getElementById('submitBtn').style.display = 'inline-block';
            }
        }

        function updateStats() {
            const answered = userAnswers.filter(a => a).length;
            const correct = userAnswers.filter(a => a && a.isCorrect).length;
            const partial = userAnswers.filter(a => a && a.isPartial).length;
            const accuracy = answered > 0 ? Math.round(((correct + partial * 0.5) / answered) * 100) : 0;

            document.getElementById('correctCount').textContent = correct + (partial > 0 ? ` (+${partial} partial)` : '');
            document.getElementById('wrongCount').textContent = answered - correct - partial;
            document.getElementById('accuracy').textContent = accuracy + '%';
        }

        function updateProgressBar() {
            const progress = ((currentQuestionIndex + 1) / questions.length) * 100;
            document.getElementById('progressBar').style.width = progress + '%';
        }

        function startTimer() {
            timer = setInterval(() => {
                const elapsed = Math.floor((Date.now() - startTime) / 1000);
                const minutes = Math.floor(elapsed / 60);
                const seconds = elapsed % 60;
                document.getElementById('timeSpent').textContent = 
                    `${minutes}:${seconds.toString().padStart(2, '0')}`;
            }, 1000);
        }

        function showReview() {
            clearInterval(timer);
            document.getElementById('studyMode').classList.add('hidden');
            document.getElementById('reviewMode').classList.remove('hidden');

            const correct = userAnswers.filter(a => a.isCorrect).length;
            const partial = userAnswers.filter(a => a.isPartial).length;
            const total = userAnswers.length;
            const accuracy = Math.round(((correct + partial * 0.5) / total) * 100);

            document.getElementById('reviewStats').innerHTML = `
                <div class="stats">
                    <div class="stat-card">
                        <div class="stat-number">${correct}/${total}</div>
                        <div>Ho√†n to√†n ƒë√∫ng</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${partial}</div>
                        <div>ƒê√∫ng m·ªôt ph·∫ßn</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">${accuracy}%</div>
                        <div>ƒêi·ªÉm t·ªïng</div>
                    </div>
                </div>
            `;

            // Show wrong answers for review
            const wrongAnswers = userAnswers.filter(a => !a.isCorrect);
            if (wrongAnswers.length > 0) {
                document.getElementById('wrongAnswers').innerHTML = `
                    <h3>üîÑ C√¢u c·∫ßn √¥n l·∫°i (${wrongAnswers.length} c√¢u):</h3>
                    ${wrongAnswers.map(a => `
                        <div class="wrong-answer">
                            <strong>C√¢u ${a.questionId} (${a.type === 'multiple' ? 'Multiple' : 'Single'} Choice):</strong><br>
                            B·∫°n ch·ªçn: ${a.selected.map(i => String.fromCharCode(65 + i)).join(', ')}<br>
                            ƒê√°p √°n ƒë√∫ng: ${a.correct.map(i => String.fromCharCode(65 + i)).join(', ')}
                            ${a.isPartial ? ' <span style="color: #856404;">(ƒê√∫ng m·ªôt ph·∫ßn)</span>' : ''}
                        </div>
                    `).join('')}
                `;
            }
        }

        function restartStudy() {
            currentQuestionIndex = 0;
            userAnswers = [];
            startTime = Date.now();
            document.getElementById('studyMode').classList.remove('hidden');
            document.getElementById('reviewMode').classList.add('hidden');
            initializeApp();
        }

        function exportResults() {
            const results = {
                date: new Date().toLocaleDateString('vi-VN'),
                day: 1,
                questions: questions.length,
                correct: userAnswers.filter(a => a.isCorrect).length,
                partial: userAnswers.filter(a => a.isPartial).length,
                accuracy: Math.round(((userAnswers.filter(a => a.isCorrect).length + userAnswers.filter(a => a.isPartial).length * 0.5) / userAnswers.length) * 100),
                wrongQuestions: userAnswers.filter(a => !a.isCorrect).map(a => ({
                    id: a.questionId,
                    type: a.type,
                    selected: a.selected,
                    correct: a.correct,
                    isPartial: a.isPartial
                })),
                timeSpent: document.getElementById('timeSpent').textContent
            };

            const blob = new Blob([JSON.stringify(results, null, 2)], {type: 'application/json'});
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `aws-devops-results-${new Date().toISOString().split('T')[0]}.json`;
            a.click();
        }

        // Initialize app when page loads
        window.onload = initializeApp;
    </script>
</body>
</html>
